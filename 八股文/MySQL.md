# MySQL

## 基础篇

### 执行一条select语句，期间发生了什么？

学习SQL的时候，大家肯定第一个先学到的就是select查询语句了，比如下面这句查询语句：

```mysql
// 在product表中，查询id=1的记录
select * from product where id = 1;
```

但是有没有想过，MySQL执行一条select查询语句，在MySQL中期间发生了什么？

带着这个问题，我们可以很好的了解MySQL内部的架构，下来我们就在拆解一下MySQL内部的结构，看看内部里的每一个”零件“具体是负责做什么的。

#### MySQL执行流程是怎样的？

MySQL的架构共分为两层：Server层和存储引擎层

- **Server层负责建立连接，分析和执行SQL。**MySQL大多数的核心功能模块都在这实现，主要包括连接器、查询缓存、解析器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函等）和所有跨存储引擎的功能（如存储过程、触发器、视图等）都在Server层实现。

- **存储引擎层负责数据的存储和提取。**支持InnoDB、MyISAM、Memory等多个存储引擎，不同的存储引擎共用一个Server层。现在最常用的存储引擎是InnoDB，从MySQL5.5版本开始，InnoDB成为了MySQL的默认存储引擎。我们常说的引擎数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不同，比如InnoDB支持索引类型是B+树，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是B+树索引。

好了，现在我们对Server层和存储引擎层有了一个简单的认识，接下来，就详细说一条SQL查询语句的执行流程，依次看看每一个功能模块的作用。

#### 第一步：连接器

如果你在Linux系统里要使用MySQL，那你第一步肯定是要先连接MySQL服务，然后才能执行SQL语句，普遍我们都是使用下面这条命令进行连接：

```shell
# -h 指定MySQL服务的IP地址，如果是本地的MySQL服务，可以不用这个参数；
# -u 指定用户名，管理员角色名为root
# -p 指定密码，如果命令行中不填写密码（为了密码安全，建议不要在命令行写密码），就需要在交互对话里输入密码
mysql -h $ip -u $user -p
```

连接的过程需要先经过TCP三次握手，因为MySQL是基于TCP协议进行传输的，如果MySQL服务并没有启动，则会收到报错。

如果MySQL服务正常运行，完成TCP连接的建立后，连接器就要开始验证你的用户名和密码，如果用户名或密码不对，就会收到一个"Access denied for user"的错误，然后客户端程序结束执行。

如果用户密码都没有问题，连接器就会获取该用户的权限，然后保存起来，后续该用户在此连接里的任何操作，都会基于连接开始时读到的权限进行权限逻辑的判断。

所以，如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

**如何查看MySQL服务被多少个客户端连接了？**

如果你想知道当前MySQL服务被多少个客户端连接了，你可以执行`show processlist`命令进行查看。

**空闲连接会一直占用着吗？**

当然不是了，MySQL定义了空闲连接的最大空闲时长，由`wait_timeout`参数控制的，默认值时8小时（28880秒），如果空闲连接超过了这个时间，连接器就会自动将它断开。

```mysql
mysql> show variables like 'wait_timeout';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| wait_timeout  | 28800 |
+---------------+-------+
1 row in set (0.00 sec)
```

当然，我们自己也可以手动断开空闲的连接，使用的是kill connection + id 的命令。

```mysql
mysql> kill connection +6;
Query OK, 0 rows affected (0.00 sec)
```

一个处于空闲状态的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。

**MySQL的连接数有限制吗？**

MySQL服务支持的最大连接数由max_connection参数控制，比如我的MySQL服务默认是151个，超过这个值，系统就会拒绝接下来的连接请求，并报错提示”Too many connections“。

```mysql
mysql> show variables like 'max_connections';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| max_connections | 151   |
+-----------------+-------+
1 row in set (0.00 sec)
```

MySQL的连接也跟HTTP一样，有短连接和长连接的概念，它们的区别如下：

```tex
// 短连接
连接mysql服务（TCP 三次握手）
执行sql
断开mysql服务（TCP 四次挥手）

// 长连接
连接mysql服务（TCP 三次握手）
执行mysql
执行mysql
执行mysql
...
断开mysql服务（TCP 四次挥手）
```

可以看到，使用长连接的好处就是可以减少建立连接和断开连接的过程，所以一般是推荐使用长连接。

但是，使用长连接后可能会占用内存增多，因为MySQL在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才释放。如果长连接累计很多，将会导致MySQL服务占用内存太大，有可能会被系统强制杀掉，这样会发生MySQL服务异常重启的现象。

**怎么解决长连接占用内存的问题？**

有两种解决方式。

第一种，定期断开长连接。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。

第二种，客户都拿主动重置连接。MySQL 5.7版本实现了`mysql_reset_connection()`函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用mysql_reset_connection函数来重置连接，达到释放内存的效果，这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

至此，连接器的工作做完了，简单总结一下：

- 与客户端进行TCP三次握手建立连接；
- 校验客户端的用户名和密码，如果用户名或密码不对，则会报错；
- 如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限；

#### 第二步：查询缓存

连接器的工作完成后，客户端就可以向MySQL服务发送SQL语句了，MySQL服务收到SQL语句后，就会解析出SQL语句的第一个字段，看看是什么类型的语句。

如果SQL是查询语句（select语句），MySQL就会先去查询缓存（Query Cache）里查询缓存数据，看看之前有没有执行过这一条命令，这个查询缓存时以key-value形式保存在内存中的，key为SQL查询语句，value为SQL语句查询的结果。

如果查询的语句命中查询缓存，那么就会直接返回value给客户端。如果查询的语句没有命中查询缓存，那么久要往下继续执行，等执行完后，查询的结果就会被存入查询缓存中。

这么看，查询缓存还挺有用，但是其实查询缓存挺鸡肋的。

对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。如果刚缓存了一个查询结果很大的数据，还没被使用的时候，刚好这个表有更新操作，查询缓存就被清空了，相当于缓存了个寂寞。

所以，MySQL 8.0版本直接将查询缓存删掉了，也就是说MySQL 8.0开始，执行一条SQL查询语句，不会再走到查询缓存阶段了。

对于MySQL 8.0之前的版本，如果想关闭查询缓存，我们可以通过将参数query_cache_type设置成DEMAND。

**TIP：这里说的查询缓存时server层的，也就是MySQL 8.0版本移除的时server层的查询缓存，并不是InnoDB存储引擎中的buffer pool。**

#### 第三步：解析SQL

在正式执行SQL查询语句前，MySQL会先对SQL语句做解析，这个工作交由`解析器`完成。

##### 解析器

解析器会做如下两件事情。

第一件事情，词法分析。MySQL会根据你输入的字符串识别出关键字来，构建出SQL语法树，这样方便后面模块获取SQL类型、表名、字段名、where条件等等。

第二件事情，语法分析。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。

如果我们输入的SQL语句语法不对，就会在解析器这个阶段某错。比如，在一条查询语句中，把from写成了form，这时MySQL解析器就会给报错。

但是注意，表不存在或者字段不存在，并不是解析器里做的，《MySQL 45讲》说是在解析器做的，但是通过MySQL源码（5.7和8.0）分析得出的结论是分析器只负责构建语法树和检查语法，但是不会去查表或者字段存不存在。

那到底谁来做检测表和字段是否存在的工作呢？别着急，接下来就是了。

#### 第四步：执行SQL

经过解析器后，接着就要进入执行SQL查询语句的流程了，每条`SELECT`查询语句流程主要可以分为下面这三个阶段：

- prepare阶段，也就是预处理阶段；
- optimize阶段，也就是优化阶段；
- execute阶段，也就是执行阶段；

##### 预处理器

我们先来说说预处理阶段做了什么事情。

- 检查SQL查询语句中的表或者字段是否存在；
- 将`select *`中的`*`符号，扩展为表上的所有列；

我下面这条查询语句，test这张表是不存在的，这时MySQL就会在执行SQL查询语句的prepare阶段中报错。

```mysql
mysql> select * from test;
ERROR 1146 (42S02): Table 'mysql.test' doesn't exist
```

#### 优化器

经过预处理阶段后，还需要为SQL查询语句先制定一个执行计划，这个工作交由`优化器`来完成的。

**优化器主要负责将SQL查询语句的执行方案确定下来，**比如在表里面有很多个索引的时候，优化器会基于查询成本的考虑，来决定使用哪个索引。

当然，我们本次的查询语句（select * from product where id = 1 ）很简单，就是选择使用主键索引。

要想知道优化器选择了哪个索引，我们可以在查询语句最前面加个`explain`命令，这样就会输出这条SQL语句的执行计划，然后执行计划中的key就表示执行过程中使用了哪个索引。

如果查询语句的执行计划里的key为null说明没有使用索引，那么就会全表扫描（type = ALL），这种查询扫描的方式是效率最低档次的。

比如product表只有一个索引就是主键，现在在表中将name设置为普通索引（二级索引）。

这时product表就有主键索引（id）和普通索引（name）。假设执行了这条查询语句：

```mysql
select id from product where id > 1 and name like 'i%';
```

这条查询语句的结果既可以使用主键索引，也可以使用普通索引，但是执行的效率会不同。这时，就需要优化器来决定使用哪个索引了。

很显然这条查询语句是**覆盖索引**，直接在二级索引就能查到结果（因为二级索引的B+树的叶子节点的数据存储的是主键值），就没必要再去主键索引查找了，因为查询主键索引的B+树的成本会比查询二级索引B+树的成本大，优化器基于查询成本的考虑，会选择查询代价小的普通索引。

使用执行计划时，我们可以看到，执行过程中使用了普通索引（name），Exta为Using index，这就是表明使用了覆盖优化索引。

#### 执行器

经历完优化器后，就确定了执行方案，接下来MySQL就真正执行语句了，这个工作是由`执行器`完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。

接下来，就用三种方式执行过程，跟大家说一下执行器和存储引擎的交互过程。

- 主键索引查询
- 全表扫描
- 索引下推

**主键索引查询**

以本文开头查询语句为例，看看执行器是怎么工作的。

```mysql
select * from product where id = 1;
```

这条查询语句的查询条件用到了主键索引，而且是等值查询，同时唯一主键id是唯一，不会有id相同的记录，所以优化器决定选用访问类型为const进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的：

- 执行器第一次查询，会调用read_first_record函数指针指向的函数，因为优化器选择的访问类型为const，这个函数指针被指向为InnoDB引擎索引查询的接口，把条件`id = 1`交给存储引擎，**让存储引擎定位符合条件的第一条记录**。
- 存储引擎通过主键索引的B+树结构定位到id=1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器。

- 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。
- 执行器查询的过程是一个while循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用read_record函数指针指向的函数，因为优化器选择的访问类型为const，这个函数指针被指向为一个永远返回-1的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。

至此，这个语句就执行完成了。

**全表扫描**

举个全表扫描的例子：

```mysql
select * from product where name = 'iphone';
```

这条查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为ALL进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的：

- 执行器第一次查询，会调用read_first_record函数指针指向的函数，因为优化器选择的访问类型为all，这个函数指针被指向为InnoDB引擎全扫描的接口，**让存储引擎读取表中的第一条记录**；
- 执行器会判断读到的这条记录的name是不是iphone，如果不是则跳过；如果是则将记录发送给客户端（没错，Server层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端时等查询语句查询完成后，才会显示出所有的记录）。
- 执行器查询的过程是一个while循环，所以还会再查一次，会调用read_record函数指针指向的函数，因为优化器选择的访问类型为all，read_record函数指针指向的还是InnoDB引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行层（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端；
- 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层）返回了读取完毕的信息；
- 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。

至此，这个语句就执行完成了。

**索引下推**

在这部分非常适合将索引下推（MySQL 5.6推出了查询优化策略），这样大家都能清楚得知道，`下推`这个动作，下推到了哪里。

索引下推能够减少**二级索引**在查询时的回表操作，提高查询的效率，因为它将Server层部分负责的事情，交给存储引擎层去处理了。

举一个例子，方便大家理解，有一张用户表，对age和reward字段建立了联合索引（age, reward），

现在有下面这条查询语句：

```mysql
select * from t_user where age > 20 and reward = 10000;
```

联合索引当遇到范围查询（>、<）就会停止匹配，也就是age字段能用到联合索引，但是reward字段则无法利用到联合索引。

那么，不使用索引下推（MySQL 5.6之前的版本）时，执行器与存储引擎的执行流程是这样的：

- Server层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到age > 20 的第一条记录；
- 存储引擎根据二级索引的B+树快速定位到这条记录后，获取主键值，然后进行回表操作，将完整的记录返回给Server层；
- Server层在判断该记录的reward是否等于100000，如果成立则将其发送给客户端；否则跳过该记录；
- 接着，继续向存储引擎搜索下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给Server层；
- 如此往复，直到存储引擎把表中的所有记录读完。

可以看到，没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给Server，接着Server再判断该记录的reward是否等于100000。

而使用索引下推后，判断记录的reward是否等于100000的工作交给了存储引擎层，过程如下：

- Server层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到age > 20 的第一条记录；
- 存储引擎定位到二级索引后，先不执行回表操作，而是判断一下该索引中包含的列（reward列）的条件（reward是否等于100000）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给Server层。
- Server层在判断其他查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。
- 如此反复，直到存储引擎把表中的所有记录读完。

可以看到，使用了所有下推后，虽然reward列无法使用联合索引，但是因为它包含在联合索引（age， reward）里，所以直接在存储引擎过滤出满足reward = 100000的记录后，才会执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。

当你发现执行计划里的Extra部分显示了”Using index condition“，说明使用了索引下推。

#### 总结

执行一条SQL查询语句，期间发生了什么？

- 连接器：建立连接，管理连接，校验用户身份；
- 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0已删除该模块；
- 解析SQL：通过解析器对SQL查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；
- 执行SQL：执行SQL共有三个阶段：
  - 预处理阶段：检查表或字段是否存在；将`select *`中的`*`符号扩展为表上的所有列。
  - 优化阶段：基于查询成本的考虑，选择查询成本最小的执行计划；
  - 执行阶段：根据执行计划执行SQL查询语句，从存储引擎读取记录，返回给客户端；

### MySQL一行记录是怎么存储的？

#### MySQL的数据存放在哪个文件？

大家都知道MySQL的数据是保存在磁盘的，那具体是保存在哪个文件呢？

MySQL存储的行为是由存储引擎实现的，MySQL支持多种存储引擎，不同的存储引擎保存的文件自然也不同。

InnoDB是我们常用的存储引擎，也是MySQL默认的存储引擎，所以，本文主要以InnoDB存储引擎展开讨论。

先来看看MySQL数据库的文件存放在哪个目录？

```mysql
mysql> SHOW VARIABLES LIKE 'datadir';
+---------------+-----------------+
| Variable_name | Value           |
+---------------+-----------------+
| datadir       | /var/lib/mysql/ |
+---------------+-----------------+
1 row in set (0.00 sec)
```

我们每创建一个database（数据库）都会在/var/lib/mysql目录里面创建一个以database为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。

比如现在有一个名为my_test的database，该database里有一张名为t_order数据库表。

然后我们进入/var/lib/mysql/my_test目录，看看里面有什么文件？

```shell
$ ls /var/lib/mysql/my_test
db.opt  
t_order.frm  
t_order.ibd
```

可以看到，共有三个文件，这三个文件分别代表着：

- db.opt，用来存储当前数据库的默认字符集和字符校验规则。
- t_order.frm，t_order的表结构会保存在这个问题。在MySQL中建立一张表都会生成一个.frm文件，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。
- t_order.ibd，t_order的表数据会保存在这个文件。表数据既可以存在共享表空间文件（文件名：ibdata1）里，也可以存放在独占表空间文件（文件名：表名字.ibd）。这个行为是由参数innodb_file_per_table控制的，若设置了参数innodb_file_per_table为1，则会将存储的数据、索引等信息单独存储在一个独占表空间，从MySQL 5.6版本开始，它的默认值就是1了，因此从这个版本之后，MySQL中的每一张表的数据都存放在一个独立的.idb文件。

好了，现在我们知道了一张数据库表的数据是保存在`表名字.ibd`的文件里，这个文件也称为独占表空间文件。

#####  表空间文件的结构是怎样的？

表空间由段（segment）、区（extent）、页（page）、行（row）组成。

1. 行（row）

   数据库表中的记录都是按行（row）进行存放的，每行记录根据不同的行格式，有不同的存储结构。后面我们详细介绍InnoDB存储引擎的行格式，也是本文重点介绍的内容。

2. 页（page）

   记录时按照行来存储的，但是数据库的读取并步以`行`为单位，否则一次读取（也就是一次I/O操作）只能处理一行数据，效率会非常低。

   因此，**InnoDB的数据是按`页`为单位来读写的**，也就是说，当需要读一条数据的时候，并不是将这个行数据从磁盘读出来，而是以页为单位，将其整体读入内存。

   默认每个页的大小为16KB，也就是最多能保证16KB的连续存储空间。

   页是InnoDB存储引擎磁盘管理的最小单位，意味着数据库每次读写都是以16kb为单位的，一次最少从磁盘中读取16K的内容到内存中，一次最少把内存中的16K内容刷新到磁盘中。

   页的类型有很多，常见的有数据页、undo日志页、溢出页等等。数据表中的行记录是用`数据页`来管理的。

   总之知道表中的记录存储在`数据页`里面就行。

3. 区（extent）

   我们知道InnoDB存储引擎是用B+树来组织数据的。

   B+树中每一层都是通过双向链表连接起来的，如果是以页为单位来分配存储空间，那么链表中相邻的两个页之间的物理位置并不是连续的，可能离得非常远，那么磁盘查询时就会有大量的随机I/O，随机I/O是非常慢的。

   解决这个问题也很简单，就是让链表中相邻的页的物理位置也相邻，这样就可以使用顺序I/O了，那么在范围查询（扫描叶子节点）的时候性能就会很高。

   那具体怎么解决呢？

   **在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为1MB，对于16KB的页来说，连续的64个页会被划分为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序I/O了。**

4. 段（segment）

   表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分配数据段、索引段和回滚段等。

   - 索引段：存放B+树的非叶子节点的区的集合；
   - 数据段：存放B+树的叶子节点的区的集合；
   - 回滚段：存放的是回滚数据的区的集合，后面讲事务隔离的时候就介绍到了MVCC利用了回滚段实现了多版本查询数据。

   好了，终于说完表空间的结构了。接下来，就具体讲一下InnoDB的行格式了。

   之所以要绕一大圈才能讲行记录的格式，主要是想让大家知道行记录时存储在哪个文件，以及行记录在这个表空间文件中的哪个区域，有一个从上往下切入的视角，这样理解起来不会觉得很抽象。

#### InnoDB行格式有哪些？

行格式（row_format），就是一条记录的存储结构。

InnDB提供了4种行格式，分别是Redundant、Compact、Dynamic和Compressed行格式。

- Redundant是很古老的行格式了，MySQL 5.0版本之前用的行格式了，现在基本没人用了。
- 由于Redundant不是一种紧凑的行格式，所以MySQL 5.0之后引入了Compact行记录存储方式，Compact是一种紧凑的行格式，设计的初衷就是为了让一个数据页中可以存放更多的行记录，从MySQL 5.1版本之后，行格式默认设置成Compact。
- Dynamic和Compressed两个都是紧凑的行格式，它们的行格式都和Compact差不多，因为都是基于Compact改进一点东西。从MySQL5.7版本之后，默认使用Dynamic行格式。

Redundant行格式这里我就不讲了，因为现在基本没人用了，这次重点介绍Compact行格式，因为Dynamic和Compressed这两个行格式跟Compressed非常像。

所以，弄懂了Compact行格式，之后你们在去了解其他行格式，很快也能看懂。

#### Compact行格式长什么样？

一条完整的记录分为`记录的额外信息`和`记录的真实数据`两个部分。

##### 记录的额外信息

记录的额外信息包含三个部分：变长字段长度列表、NULL值列表、记录头信息。

1. 变长字段长度列表

   varchar(n)和char(n)的区别是什么，相信大家都非常清楚，char是定长的，varchar是变长的，变长字段实际存储的数据的长度（大小）不固定的。

   所以，在存储数据的时候，也要把数据占用的大小存起来，存到`变长字段长度列表`里面，读取数据的时候才能根据这个`变长字段长度列表`去读取对于长度的数据。其他TEXT、BOLB等变长字段也是这么实现的。

   为了展示`变长字段长度列表`具体是怎么保存`变长字段的真实数据占用的字节数`，我们先创建这个一张表，字符集ascii（所以每一个字符占用的1字节），行格式是Compact，t_user表中name和phone字段都是变长字段：

   ```mysql
   CREATE TABLE `t_user` (
     `id` int(11) NOT NULL,
     `name` VARCHAR(20) DEFAULT NULL,
     `phone` VARCHAR(20) DEFAULT NULL,
     `age` int(11) DEFAULT NULL,
     PRIMARY KEY (`id`) USING BTREE
   ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT;
   ```

   现在t_user表里有三条记录，接下来，我们来看看这三条记录的行格式中的`变长字段长度列表`是怎样存储的。

   先来看第一条记录：

   - name列的值为a，真实数据占用的字节数是1字节，十六进制0x01；
   - phone列的值为123，真实数据占用的字节数是3字节，十六进制0x03；
   - age列和id列不是变长字段，所以这里不用管。

   这些变长字段的真实数据占用的字节数会按照列的顺序逆序存放（等下会说为什么要这么设计），所以`变长字段长度列表`里的内容是[03 01]，而不是[01 03]。

   **为什么`变长字段长度列表`的信息要按照逆序存放？**

   这个设计是有想法的，主要是因为`记录头信息`中指向下一个记录的指针，指向的是下一条记录的`记录头信息`和`真实数据`之间的位置，这样的好处是从左读就是记录头信息，向右读就是真实数据，比较方便。

   `变长字段长度列表`中的信息之所以要逆序存放，是因为这样可以**使得位置靠前的记录的真实数据和数据对于的字段长度信息可以同时在一个CPU Cache Line中，这样就可以提高CPU Cache的命中率。**

   同样的道理，NULL值列表的信息也需要逆序存放。

   **每个数据库表的行格式都有`变长字段字节数列表`吗**

   其实变长字段字节数列表不是必须的。

   **当数据表没有变长字段的时候，比如全部都是int类型的字段，这时候表里的行格式就不会有`变长字段长度列表`了**，因为没必要，不如去掉以节省空间。

   所以`变长字段长度列表`只出现在数据表有变长字段的时候。

2. NULL值列表

   表中的某些列可能会存储NULL值，如果把这些NULL值都放到记录的真实数据中会比较浪费空间，所以Compact行格式把这些值为NULL的列存储到NULL值列表中。

   如果存在允许NULL值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排序。

   - 二进制位的值为`1`时，代表该列的值为NULL。
   - 二进制位的值为`0`时，代表该列的值不为NULL。

   另外，NULL值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补`0`。

   

   **每个数据库表的行格式都有`NULL值列表`吗？**

   NULL值列表也不是必须的。

   **当数据库表的字段都定义成NOT NULL的时候，这时候表里的行格式就不会有NULL值列表了。**

   所以在设计数据库表的时候，通常都是建议将字段设置为NOT NULL，这样可以至少节省1字节的空间（NULL值列表至少占用1字节空间）。

   

   **`NULL值列表`是固定1字节空间吗？如果这样的话，一条记录有9个字段都是NULL，这时候怎么表示？**

   `NULL值列表`的空间不是固定1字节的。

   当一条记录有9个字段都是NULL，那么就会创建2字节空间的`NULL值列表`，以此类推。

3. 记录头信息

   记录头信息中包含的内容很多，我就不一一举例了，这里说几个比较重要的：

   - delete_mask：标识此条数据是否被删除。从这里可以知道，我们执行delete删除记录的时候，并不会真正的删除记录，只是将这个记录的delete_mask标记位1。
   - next_record：下一条记录的位置。从这里可以知道，记录与记录之间是通过链表组织的。在前面也提到了，指向的是下一条记录的`记录头信息`和`真实数据`之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。
   - record_type：表示当前记录的类型，0表示普通记录，1表示B+数非叶子节点记录，3表示最小记录，3表示最大记录

##### 记录的真实数据

   记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别位：row_id、trx_id、roll_pointer，我们来看下这三个字段是什么。

   - row_id

     如果我们建表的时候指定了主键或者唯一约束列，那么就没有row_id隐藏字段了。如果既没有指定主键，又没有唯一约束，那么InnoDB就会为记录添加row_id隐藏字段。row_id不是必须的，占用6个字节。

   - trx_id

     事务id，表示这个数据是由哪个事务生成的。trx_id是必须的，占用6个字节。

   - row_pointer

     这条记录上一个版本的指针。roll_pointer是必须的，占用7个字节。

     如果你熟悉MVCC机制，你应该就清楚trx_id和roll_pointer的作用了，接着往下看，MVCC一定要掌握，面试经常问。
     
#### varchar(n)中n最大取值为多少？

   我们要清楚一点，**MySQL规定除了TEXT、BLOBs这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过65535个字节。**

   也就是说，一行记录除了TEXT、BLOBs类型的列，限制最大为65535字节，注意是一行的总长度，不是一列。

   知道这个前提之后，我们再来看看这个问题：`varchar(n)中n最大取值为多少？`

   varchar(n)字段类型的n代表的是最多存储的字符数量，并不是字节大小。

   要算varchar(n)最大能允许存储的字节数，还要看数据库表的字符集，因为字符集代表着，1个字符要占多少字节，比如ascii字符集，1个字符占用1字节，那么varchar(n)意味着最大能允许存储100字节的数据。

##### 单字段的情况

   前面我们知道了，一行记录最大只能存储65535字节的数据。

   那假设数据库表只有一个varchar(n)类型的列且字符集是ascii，在这种情况下，varchar(n)中n最大取值是65535吗？

   不着急说结论，我们先来做个实验验证一下。

   我们定义一个varchar(65535)类型的字段，字符集为ascii的数据库表。

```mysql
   CREATE TABLE test ( 
   `name` VARCHAR(65535)  NULL
   ) ENGINE = InnoDB DEFAULT CHARACTER SET = ascii ROW_FORMAT = COMPACT;
```

   结果是创建失败了，得到的报错信息是`一行数据的最大字节数是65535（不包含TEXT、BLOBs这种大对象类型），其中包含了storage overhead`。

   问题来了，这个storage overhead是什么呢？其实就是`变长字段长度列表`和`NULL值列表`，也就是说**一行数据的最大字节数65535，其实是包含`变长字段长度列表`和`NULL值列表`所占用的字节数。**所以，我们在算varchar(n)最大值时，需要减去storage overhead占用的字节数。

   这是因为我们存储字段累心为varchar(n)的数据时，其实分成了三个部分来存储：

   - 真实数据
   - 真实数据占用的字节数
   - NULL标识，如果不允许为NULL，这部分不需要

   **本次案例中，`NULL值列表`所占用的字节数是多少？**

   前面我们创建表的时候，字段是允许为NULL的，所以**会用1字节来表示`NULL值列表`**。

   **本次案例中，`变长字段长度列表`所占用的字节数是多少？**

   `变长字段长度列表`所占用的字节数=所有`变长字段长度`占用的字节数之和。

   所以，我们要先知道每个变长字段的`变长字段长度`需要用多少字节表示？具体情况分为：

   - 条件一：如果变长字段允许存储的最大字节数小于等于255字节，就会用1字节表示`变长字段长度`；
   - 条件二：如果变长字段允许存储的最大字节数大于255字节，就会用2字节表示`变长字段长度`；

   我们这里字段类型时varchar(65535)，字符集是ascii，所以代表着变长字段允许存储的最大字节数是65535，符合条件二，所以会用2字节来表示`变长字段长度`。

   **因为我们这个案例时只有一个变长字段，所以`变长字段长度列表`=1个`变长字段长度`占用的字节数，也就是2字节**

   因为我们在算varchar(n)最大值时，需要减去`变长字段长度列表长度`和`NULL值列表`所占用的字节数。所以，**在数据库表中有一个varchar(n)字段且字符集时ascii的情况下，varchar(n)最大值=65535-2-1=65532。**

   在算varchar(n)最大值时，需要减去`变长字段长度列表`和`NULL值列表`所占用的字节数。

   如果采用的是UTF-8，varchar(n)最多能存储的数据计算方式就不一样了：

   - 在UTF-8字符集下，一个字符串最多需要三个字节，varchar(n)的n最大取值就是65532/3=21844。

#####    多字段的情况

   **如果有多个字段的话，要保证所有字段的长度+变长字段字节数列表所占用的字节数+NULL值列表所占用的字节数<=65535**

#### 行溢出后,MySQL是怎么处理的?

MySQL中磁盘和内存交互的基本单位是页，一个页的大小一般是`16KB`，也就是`16384`字节，而一个varchar(n)类型的列最多可以存储`65531字节`，一些大对象如TEXT、BLOB可能存储更多的数据，这时一个页可能就存不了一条记录。这个时候就会**发生行溢出，多的数据就会存到另外的`溢出页`中。**

如果一个数据页存不了一条记录，InnoDB存储引擎会自动将溢出的数据存放到`溢出页`中。一般情况下，InnoDB的数据都是存放在`数据页`中。但是发生行溢出时，溢出的数据会存放到`溢出页`中。

当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在`溢出页`中，然后真实数据处用20字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。

上面这个时Compact行格式在发生行溢出后的处理。

Compressed和Dynamic这两个行格式和Compact非常类似，主要的区别在于处理行溢出数据时有些区别。

这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的部分数据，只存储20个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。

#### 总结

**MySQL的NULL值是怎么存放的？**

MySQL的Compact行格式中会用`NULL值列表`来标记NULL的列，NULL值并不会存储在行格式中的真实数据部分。

NULL值列表会占用1字节空间，当表中所有字段都定义成NOT NULL，行格式中就不会有NULL值列表，这样就可以节省1字节的空间。

**MySQL怎么知道varchar(n)实际占用数据的大小？**

MySQL的Compact行格式中会用`可变长字段长度列表`存储变长字段实际占用的数据大小。

**varchar(n)中n最大取值为多少？**

一行记录最大能存储65535字节的数据，但是这个是包含`变长字段字节数列表所占用的字节数`和`NULL值列表所占用的字节数`。所以，我们在计算varchar(n)中n最大值时，需要减去这两个列表所占用的字节数。

如果一张表只有一个varchar(n)字段，且允许为NULL。字符集为ascii。varchar(n)中n最大取值为65532。

计算公式：65535-变长字段字节数列表所占用的字节数-NULL值列表所占用的字节数=65535-2-1=65532。

如果有多个字段的话，要保证所有字段的长度+变长字段字节数列表所占用的字节数+NULL值列表占用的字节数<=65535。

**行溢出后，MySQL是怎么处理的？**

如果一个数据页存不了一条记录，InnoDB存储引擎会自动将溢出的数据存放到`溢出页`中。

Compact行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据出只会保存该列的一部分数据，而把剩余的数据放在`溢出页`中，然后真实数据处用20字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。

Compressed和Dynamic这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储20个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。

**参考资料：**

- 《MySQL是怎样运行的》
- 《MySQL技术内部InnoDB存储引擎》

## 索引篇

### 索引常见面试题

面试中，MySQL索引相关的问题基本上都是一系列问题，都是先从索引的基本原理，再到索引的使用场景，比如：

- 索引底层使用了什么数据结构和算法？
- 为什么MySQL InnoDB选择B+tree作为索引的数据结构？
- 什么时候适用索引？
- 什么时候不需要创建索引？
- 什么情况下索引会失效？
- 有什么优化索引的方法？
- ......

今天就带大家，夯实MySQL索引的知识点。

#### 什么是索引？

当你想查阅书中某个知识的内容，你会选择一页一页的找呢？还是在书的目录去找呢？

傻瓜都知道时间是宝贵的，当然是选择在书的目录去找，找到后再翻到对应的页。书中的**目录**，就是充当**索引**的角色，方便我们快速查找书中的内容，所以索引是以空间换时间的设计思想。

那换到数据库中，索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就是**索引是数据的目录**。

索引的存储索引，说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方式。MySQL存储索引有MyISAM、InnoDB、Memory，其中InnoDB是在MySQL5.5之后称为默认的存储索引。

#### 索引的分类

你知道索引有哪些吗？大家肯定都能劈里啪啦地说出聚簇索引、主键索引、二级索引、普通索引、唯一索引、hash索引、B+树索引等等。

然后在问你，你能将这些索引分一下类吗？可能大家就有点模糊了。其实，要对这些索引进行分类，要清楚这些索引的使用和实现方式，然后再针对有相同特征的索引归为一类。

我们可以按照四个角度来分类索引。

- 按`数据结构`分类：B+tree索引、Hash索引、Full-text索引。
- 按`物理存储`分类：聚簇索引（主键索引）、二级索引（辅助索引）。
- 按`字段特征`分类：主键索引、唯一索引、普通索引、前缀索引。
- 按`字段个数`分类：单列索引、联合索引。

接下来，按照这些角度来说说各类索引的特点。

##### 按数据结构分类

从数据结构的角度来看，MySQL常见索引有B+Tree索引、Hash索引、Full-Text索引。

每一种存储引擎支持的索引类型不一定相同，我在表中总结了MySQL常见的存储引擎InnoDB、MyISAM和Memory分别支持的索引类型。

| 索引类型      | InnoDB引擎                                                 | MyISAM引擎 | Memory引擎 |
| ------------- | ---------------------------------------------------------- | ---------- | ---------- |
| B+Tree索引    | Yes                                                        | Yes        | Yes        |
| Hash索引      | No（不支持hash索引，但是在内存结构中有一个自适应hash索引） | No         | Yes        |
| Full-Text索引 | Yes（MySQL 5.6版本后支持）                                 | Yes        | No         |

InnoDB在MySQL 5.5之后称为默认的MySQL存储引擎，B+Tree索引类型也是MySQL存储引擎采用最多的索引类型。

在创建表时，InnoDB存储引擎会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键（key）；
- 如果没有主键，就选择第一个不包含NULL值的唯一列作为聚簇所以的索引键（key）；
- 在上面两个都没有的情况下，InnoDB将自动生成一个隐式自增id列作为聚簇索引的索引键（key）；

其他所有都属于辅助索引（Secondary Index），也被称为二级索引或非聚簇索引。**创建的主键索引和二级索引默认使用的是B+Tree索引**。

为了让大家理解B+Tree索引的存储和查询的过程，接下来通过一个简单的例子，说明一下B+Tree索引在存储数据中的具体实现。

先创建一张商品表，id为主键，如下：

```mysql
CREATE TABLE `product`  (
  `id` int(11) NOT NULL,
  `product_no` varchar(20)  DEFAULT NULL,
  `name` varchar(255) DEFAULT NULL,
  `price` decimal(10, 2) DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE
) CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;
```

然后再商品表里插入一些行数据，这些行数据存储再B+Tree索引时是长什么样子的？

B+Tree是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，而且每个节点里的数据时按**主键顺序存放**的。每一层父节点的索引值都会出现再下层子节点的索引值中，因此再叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。

**通过主键查询商品数据的过程**

比如，我们执行了下面这条查询语句：

```mysql
select * from product where id = 5;
```

这条语句使用了主键索引查询id号为5的商品。查询过程是这样的，B+Tree会自顶向下逐层进行查找；

- 将5与根节点的索引数据（1，10，20）比较，5在1和10之间，所以根据B+Tree的搜索逻辑，找到第二层的所有数据（1，4，7）；
- 在第二层的索引数据（1，4，7）中进行查找，因为5在4和7之间，所以找到第三层的所有数据（4，5，6）；
- 在叶子节点的索引数据（4，5，6）中进行查找，然后我们找到了索引值为5的行数据。

数据库的索引和数据都存储在硬盘的，我们可以把一次读取一个节点当作一次磁盘I/O操作。那么上面的整个查询过程一共经历了3个节点，也就是进行了3次I/O操作。

B+Tree存储千万级的数据只需要3-4层高度就可以满足，这意味着从千万级的表查询目标数据最多需要3-4次磁盘I/O，所以**B+Tree相比于B树和二叉树来说，最大的优势在于查询效率很高，因为即使在数据量很大的情况下，查询一个数据的磁盘I/O依然维持在3-4次**。

**通过二级索引查询商品数据的过程**

主键索引的B+Tree和二级索引的B+Tree区别如下：

- 主键索引的B+Tree的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的B+Tree的叶子节点里；
- 二级索引的B+Tree的叶子节点存放的是主键值，而不是实际数据。

如果我用product_no二级索引查询商品，如下查询语句：

```mysql
select * from product where product_no = '0002';
```

会先检索二级索引中的B+Tree的索引值（商品编码，product_no），找到对应的叶子节点，然后获取主键值，然后再通过主键索引中的B+Tree查询对应的叶子节点，然后获取整行数据。**这个过程交`回表`，也就是说要查两个B+Tree才能查到数据。**

不过，当查询的数据是能在二级索引的B+Tree的叶子节点里查询到，这时就不再用查询主键索引查了，比如下面这条查询语句：

```mysql
select id from product where product_no = '0002';
```

**这种在二级索引的B+Tree就能查询到结果的过程叫作`覆盖索引`，也就是只需要查一个B+Tree就能找到数据**。



**为什么MySQL InnoDB选择B+Tree作为索引的数据结构？**

前面已经讲了B+Tree的索引原理，现在就来回答一下B+Tree相比于B树、二叉树或Hash结构的优势在哪？

1. B+Tree vs B Tree

   B+Tree只在叶子节点存储数据，而B树的非叶子节点也要存储数据，所以B+Tree的单个节点的数据量更小，在相同的磁盘I/O次数下，就能查询更多的节点。

   另外，B+Tree叶子节点采用的是双向链表连接，适合MySQL中常见的基于范围的顺序查找，而B树无法做到这一点。

2. B+Tree vs 二叉树

   对于有N个叶子节点的B+Tree，其搜索复杂度为`O(logdN)`，其中d表示节点允许的最大子节点个数为d个。

   在实际的应用当中，d值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree的高度依然维持在3~4层左右，也就是说一次数据查询操作只需要做3~4次的磁盘I/O操作就能查询到目标数据。

   而二叉树的每个父节点的儿子节点个数只能时2个，意味着其搜索复杂度为`O(logN)`，这个已经比B+Tree高出不少，因此二叉树检索到目标数据所经历的磁盘I/O次数要更多。

3. B+Tree vs Hash

   Hash在做等值查询的适合效率贼快，搜索复杂度为O(1)。

   但是Hash表不适合做范围查询，它更适合做等值的查询，这也是B+Tree索引要比Hash表索引有着更广泛的适用场景的原因。

##### 按物理存储分类

从物理存储的角度来看，索引分为聚簇索引（主键索引）、二级索引（辅助索引）。

这两个区别在前面也提到了：

- 主键索引的B+Tree的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的B+Tree的叶子节点里；
- 二级索引的B+Tree的叶子节点存放的是主键值，而不是实际数据。

所以，在查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是覆盖索引。如果查询的数据不在二级索引里。就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表。

##### 按字段特性分配

从字段特性角度来看，索引分为主键索引、唯一索引、普通索引、前缀索引。

**主键索引**

主键索引就是建立在主键字段上的索引，通常在创建表的时候就一起创建，一张表最多只有一个主键索引，索引列的值不允许有空值。

在创建表时，创建主键索引的方式如下：

```mysql
CREATE TABLE table_name  (
  ....
  PRIMARY KEY (index_column_1) USING BTREE
);
```

**唯一索引**

唯一索引建立在UNIQUE字段上的索引，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值。

在创建表时，创建唯一索引的方式如下：

```mysql
CREATE TABLE table_name  (
  ....
  UNIQUE KEY(index_column_1,index_column_2,...) 
);
```

建表后，如果要创建唯一索引，可以使用下面这条命令：

```mysql
CREATE UNIQUE INDEX index_name
ON table_name(index_column_1,index_column_2,...); 
```

**普通索引**

普通索引就是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为UNIQUE。

在创建表时，创建普通索引的方式如下：

```mysql
CREATE TABLE table_name  (
  ....
  INDEX(index_column_1,index_column_2,...) 
);
```

建表后，如果要创建普通索引，可以使用下面这条命令：

```mysql
CREATE INDEX index_name
ON table_name(index_column_1,index_column_2,...); 
```

**前缀索引**

前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立索引，前缀索引可以建立在字段类型为char、varhcar、binary、varbinary的列上。

使用前缀索引的目的时为了减少索引占用的存储空间，提升查询效率。

在创建表时，创建前缀索引的方式如下：

```mysql
CREATE TABLE table_name(
    column_list,
    INDEX(column_name(length))
); 
```

建表后，如果要创建前缀索引，可以使用下面这条命令：

```mysql
CREATE INDEX index_name
ON table_name(column_name(length)); 
```

##### 按字段个数分类

从字段个数的角度来看，索引分为单列索引、联合索引（复合索引）。

- 建立在单列上的索引称为单列索引，比如主键索引；
- 建立在多列上的索引称为联合索引；

**联合索引**

通过将多个字段组合成一个索引，该索引就被称为联合索引。

比如，将商品表中的product_no和name字段组合成联合索引`(product_no, name)`，创建联合索引的方式如下：

```mysql
CREATE INDEX index_product_no_name ON product(product_no, name);
```

联合索引的非叶子节点占用两个字段的值作为B+Tree的key值。当在联合索引查询数据时，先按product_no字段比较，在product_no相同的情况下再按name字段比较。

也就是说，联合索引查询的B+Tree是先按product_no进行排序，然后在product_no相同的情况再按name字段排序。

因此，使用联合索引时，存在**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配。在使用联合索引进行查询的时候，如果不遵循`最左匹配原则`，联合索引会失效，这样就无法利用到索引快速查询的特性了。

比如，如果创建了一个（a，b，c）联合索引，如果查询条件是以下这几种，就可以匹配上联合索引了：

- where a = 1;
- where a = 1 and b = 2 and c = 3;
- where a = 1 and b = 2;

需要注意的是，因为有查询优化器，所以a字段在where字句的顺序并不重要。

但是，如果查询条件是以下这几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效：

- where b = 2;
- where c = 3;
- where b = 2 and c = 3;

上面这些查询条件之所以会失效，是因为（a，b，c）联合索引，是先按a排序的，在a相同的情况再按b排序，在b相同的情况下再按c排序。所以，**b和c是全局无序，局部相对有序的**，这样在没有遵循最左匹配原则的情况下，是无法利用到索引的。

举个联合索引（a，b）的例子，a是全局有序的（1，2，2，3，4，5，6，7，8），而b是全局无序的（12，7，8，2，3，8，10，5，2）。因此，直接执行`where b = 2`这种查询条件没有办法利用联合索引的**利用索引的前提是索引里的key是有序的**。

只有在a相同的情况下，b才是有序的，必须a等于2的时候，b的值为（7，8），这时就是有序的，这个有序状态时局部的，因此，执行`where a = 2 and b = 7`是a和b字段能用到联合索引的，也就是联合索引生效了。

**联合索引范围查询**

联合索引有一些特殊的情况，**并不是查询过程使用了联合索引查询，就代表联合索引中的所有字段都用到了联合索引进行索引查询**，也就是可能存在部分字段用到联合索引的B+Tree，部分字段没有用到联合索引B+Tree的情况。

这种特殊情况就发生在范围查询。联合索引的最左匹配原则会一直向右匹配直到遇到`范围查询`就会停止匹配。**也就是范围查询的字段可以用到联合索引，但是在范围查询字段后面的字段无法用到联合索引**。

范围查询有很多种，那到底是哪些范围查询会导致联合索引的最左匹配原则会停止匹配呢？

接下俩，举例几个范围查例子。



**Q1：select * from t_table where a >1 and b = 2，联合索引（a，b）哪一个字段用到了联合索引的B+Tree？**

由于联合索引（二级索引）是先按照a字段的值排序的，所以符合a>1条件的二级索引记录肯定是相邻，于是在进行索引扫描的时候，可以定位到符合a>1条件的第一条记录，然后沿着记录所在的链表向后扫描，直到某条记录不符合a>1条件位置。所以a字段可以在联合索引的B+Tree中进行索引查询。

**但是在符合a>1条件的二级索引记录的范围里，b字段的值是无序的。**下面三条记录的a字段的值都符合a>1查询条件，而b字段的值是无序的：

- a字段值为5的记录，该记录的b字段值为8；
- a字段值为6的记录，该记录的b字段的值为10；
- a字段值为7的记录，该记录的b字段值为5；

因此，我们不能根据查询条件b=2来进一步减少需要扫描的记录数量（b字段无法利用联合索引进行索引查询的意思）。

所以在执行Q1这条查询语句的时候，对应的扫描区间是(2，+∞)，形成该扫描区间的边界条件是a>1，与b=2无关。

因此，**Q1这条查询语句只有a字段用到了联合索引进行索引查询，而b字段并没有使用到联合索引**。

我们也可以在执行计划中的key_len直到这一点，在使用联合索引进行查询的时候，通过key_len我们可以知道优化器具体使用了多少个字段的搜索条件来形成扫描区间的边界条件。

举个例子，a和b都是int类型且不为NULL的字段，那么Q1查询这条语句执行计划如下，可以看到key_len为4字节（如果字段允许为NULL，就在字段类型占用的字节数上加1，也就是5字节），说明只有a字段用到了联合索引进行索引查询，而且可以看到，即使b字段没使用联合索引，key为idx_a_b，说明Q1查询语句使用了idx_a_b联合索引。

通过Q1查询语句我们可以知道，a字段使用了>进行范围查询，联合索引的最左侧匹配原则在遇到a字段的范围查询（>）后就停止匹配了，因此b字段并没有使用到联合索引。



**Q2：select * from t_table where a >=1 and b = 2，联合索引（a，b）哪个字段用到了联合索引的B+Tree？**

Q2和Q1的查询语句很像，唯一的区别就是a字段的查询条件`大于等于`。

由于联合索引（二级索引）是先按照a字段的值排序的，所以符合>=1条件的二级索引记录肯定是相邻，于是在进行索引扫描的时候，可以定位到符合>=1条件的第一条记录，然后沿着记录所在的链表向后扫描，知道某条记录不符合a>=1条件位置。所以a字段可以在联合索引的B+Tree进行索引查询。

虽然在符合a>=1条件的二级索引记录的范围里，b字段的值是`无序`的，**但是对于符合a=1的二级索引记录的范围里，b字段的值是`有序`的**（因为对于联合索引，是先按照a字段的值排序，然后再a字段的值相同的情况下，再按照b字段的值进行排序）。

于是，在确定需要扫描的二级索引的范围时，当二级索引记录的a字段值为1时，可以通过b=2条件减少需要扫描的二级索引记录范围（b字段可以利用联合索引进行索引查询的意思）。也就是说，从符合a=1and b=2条件的第一条记录开始扫描，而不需要从第一个a字段值为1的记录开始扫描。

所以，**Q2这条查询语句a和b字段都用到了联合索引进行索引查询。**

我们也可以在执行计划中的key_len知道这一点。执行计划如下，可以看到key_len为8字节，说明优化器使用了2个字段的查询条件来形成扫描区间的边界条件，也就是a和b字段都用到了联合索引进行索引查询。

通过Q2查询语句我们可以知道，虽然a字段使用了>=进行范围查询，但是联合索引的最左匹配原则并没有在遇到a字段的范围查询（>=）后就停止匹配了，b字段还是可以用到联合索引的。



**Q3：select * from t_table where a between 2 and 8 and b = 1，联合索引（a，b）哪个字段用到了联合索引的B+Tree？**

Q3查询条件中`a between 2 and 8`的意思是查询a字段的值在2和8之间的记录。不同的数据库对between...and处理方式是有差异的。在MySQL中，between包含了value1和value2的边界值，类似于>=and=<。而有的数据库则不包含value1和value2的边界值（类似于> and <）。

这里我们只讨论MySQL。由于MySQL的between包含了value1和value2的边界值，所以类似于Q2查询语句，因此**Q3这条查询语句a和b字段都用到了联合索引进行所以查询**。

我们也可以在执行计划中的key_len知道这一点。执行计划如下，可以看到key_len为8字节，说明优化器使用了2个字段的查询条件来形成扫描区的边界条件，也就是a和b字段都用到了联合索引镜像索引查询。

通过Q3查询语句我们可以知道，虽然a字段使用了between进行范围查询，但是联合索引的最左匹配原则并没有在遇到a字段的范围查询（between）后就停止匹配了，b字段还是可以用到联合索引的。



**Q4：select * from t_user where name like 'j%' and age = 22，联合索引（name，age）哪一个字段用到了联合索引的B+Tree？**

由于联合索引（二级索引）是先按照name字段的值排序的，所以前缀为'j'的name字段的二级索引记录都是相邻的，于是在进行索引扫描的时候，可以定位到符合前缀为'j'的name字段的第一条记录，然后沿着记录所在的链表向后扫描，知道某条记录的name前缀不为'j'为止。

所以a字段可以在联合索引的B+Tree中进行索引查询，形成的扫描区间是['j', k)。注意，j是闭区间。

虽然在符合前缀为'j'得name字段得二级索引记录的范围里，age字段的值是`无序`的，**但是对于符合name=j的二级索引记录的范围里，age字段的值是`有序`的**（因为对于联合索引，是先按照name字段的值排序，然后在name字段的值相同的情况下，再按照age字段的值进行排序）。

于是，在确定需要扫描的二级索引的范围时，当二级索引的记录的name字段值为'j'时，可以通过age=22条件减少需要扫描的二级索引记录范围（age字段可以利用联合索引进行索引查询的意思）。也就是说，从符合`name='j' and age = 22`条件的第一条记录时开始扫描，而不需要从第一个name为j的记录开始扫描。

所以，**Q4这条查询语句a和b字段都用到了联合索引进行索引查询**。

我们也可以在执行计划中的key_len知道这一点。本次例子中：

- name字段的类型时varchar(30)且部位NULL，数据库表使用了utf8mb4字符集，一个字符集为utf8mb4的字符是4个字节，因此name字段的实际数据最多占用的存储空间长度是120字节（30 x 4），然后因为name是变长类型字段，需要再加2字节（用于存储该字段实际数据的长度值），也就是name的key_len为122。
- age字段的类型是int且不为NULL，key_len为4。



**Tip：可能有的同学对于`因为name是变长类型的字段，需要再加2字节`这句话有疑问。之前说过`如果变长字段允许存储的最大字节数小于等于255字节，就会用1字节表示变长字段的长度`，而这里为什么是2字节？**

**key_len的显示比较特殊，行格式是由Innodb存储引擎实现的，而执行计划是在server层生成的，所以它不会去问InnoDB存储引擎可变字段的长度占用多少字节，而是不管三七二十一都使用2字节表示可变字段的长度。**

**毕竟key_len的目的只是为了告诉你索引查询中用了哪些索引字段，而不是为了准确告诉这个字段占用多少字节空间。**



Q4查询语句的执行计划，可以得知key_len为126字节，name的key_len为122，age的key_len为4，说明优化器使用了2个字段的查询条件来形成扫描区间的边界条件，也就是name和age字段都用到了联合索引进行索引查询。

通过Q4查询语句我们可以知道，虽然name字段使用了like前缀匹配进行范围查询，但是联合索引的最左匹配原则并没有在遇到name字段的范围查询（like 'j%'）后就停止匹配了，age字段还是可以用到联合索引的。



综上所述，**联合索引的最左匹配原则，在遇到范围查询（如>、<）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段后的字段无法用到联合索引。注意，对于>=、<=、BETWEEN、like前缀匹配的范围查询，并不会停止匹配，前面也用了四个例子说明了。**



**索引下推**

现在我们知道，对于联合索引（a，b），在执行`select * from table where a > 1 and b = 2`语句的时候，只有a字段能用到索引，那么在联合索引的B+Tree找到第一个满足条件的主键值（ID为2）后，还需要判断其他条件是否满足（看b是否等于2），那是在联合索引里判断？还是回主键索引去判断呢？

- 在MySQL 5.6之前，只能从ID2（主键值）开始一个个回表，到`主键索引`上找出数据行，再对比b字段值；
- 而MySQL5.6引入的索引下推优化（index condition pushdown），**可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数**。

当你的查询语句的执行计划里，现在了Extra为`Using index condition`，那么说明使用了索引下推的优化。

**索引区分度**

另外，建立联合索引时的字段顺序，对索引效率也有很大影响。越靠前的字段被用于索引过滤的概率就越高，实际开发工作中**建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段约有可能被更多的SQL使用到**。

区分度就是某个字段column不同值得个数`除以`表的总行数。

比如，性别的区分度就很小，不适合建立索引或不适合排在联合索引列靠前的位置，而UUID这类字段就比较适合做索引或排在联合索引列的靠前的位置。

因为如果索引的区分度很小，假设字段的值分布均匀，那么无论搜索哪个值都可能达到一半的数据。在这些情况下，还不如不要索引，因为MySQL还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界限是”30%“）很高的时候，它一般会忽略索引，进行全表扫描。

**联合索引进行排序**

这里出一个题目，针对下面这条SQL，你怎么通过索引来提高查询效率呢？

```mysql
select * from order where status = 1 order by create_time asc
```

有的同学会认为，单独给status建立一个索引就可以了。

但是更好的方式是给status和create_time列建立一个联合索引，因为这样可以避免MySQL数据库发生文件排序。

因为在查询时，如果只用到status的索引，但是这条语句还要对create_time排序，这时就要用文件排序filesort，也就是在SQL执行计划中，Extra列会出现Using filesort。

所以，要利用所以的有序性，在status和create_time列建立联合索引，这样根据status筛选后的数据就是按照create_time排好序的，避免在文件排序，提高了查询效率。

#### 什么时候需要/不需要创建索引？

索引最大的好处是提高查询速度，但是所有也是有缺点的，比如：

- 需要占用物理空间，数量越大，占用空间越大；
- 创建索引和维护索引要耗费时间，这种事件随着数据量的增加而增大；
- 会降低表的增删改的效率，因为每次增删改索引，B+树为了维护索引有序性，都需要进行动态维护。

所以，索引不是万能钥匙，它也是根据场景来使用的。

**什么时候适用索引？**

- 字段有唯一性限制的，比如商品编码；
- 经常用于`where`查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
- 经常用于`group by`和`order by`的字段，这样在查询的时候就不需要再做一次排序了，因为我们都已经知道建立索引之后在B+Tree中的记录都是排序好的。

**什么时候不需要创建索引？**

- `where`条件，`group by`，`group by`里面用不到的字段，索引的价值就是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用内存物理空间的。
- 字段中存在大量重复数据，不需要创建索引，比如性别字段，只有男女，如果数据库表中，男女的记录分布均匀，那么无论搜索哪个值都可能得到一半的数据。在这些情况下，还不如不要索引，因为MySQL还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。
- 表数据太少的时候，不需要创建索引；
- 经常更新的字段不需要创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的。

#### 有什么优化索引的方法？

这里说一下几种常见优化索引的方法：

- 前缀索引优化；
- 覆盖索引优化；
- 主键索引最好是自增的；
- 防止索引失效；

##### 前缀索引优化

前缀索引顾名思义就是使用某个字段中字符串的前几个字符建立索引，那么我们为什么需要使用哦前缀来建立索引呢？

使用前缀索引是为了减小索引字段的大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。有些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。

不过，前缀索引有一定的局限性，例如：

- order by 就无法使用前缀索引；
- 无法把前缀索引作为覆盖索引；

##### 覆盖索引优化

覆盖索引是指SQL中query的所有字段，在索引B+Tree的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表操作。

假设我们只需要查询商品的名称、价格，有什么方式可以避免回表呢？

我们可以建立一个联合索引，即`商品ID、名称、价格`作为一个联合索引。如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。

所以，使用覆盖索引的好处就是，不需要查询处包含整行记录的所有信息，也就减少了大量的I/O操作。

##### 主键索引最好是自增的

我们在建表的时候，都会默认将主键索引设置为自增的，具体为什么要这么做呢？又有什么好处？

InnoDB创建主键索引默认为聚簇索引，数据被存放在了B+Tree的叶子节点上。也就是说，同一个叶子节点内的各个数据都是按主键顺序存放的，因此，每当有一条新的数据插入时，数据库会根据主键将其插入到对于的叶子节点中。

**如果我们使用自增主键**，那么每次插入的新数据就会按照顺序添加到当前所有节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面，因为每次**插入一条新纪录，都是追加操作，不需要重新移动数据**，因此这种插入数据的方法效率非常高。

**如果我们使用非自增主键**，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其他数据来满足新数据的插入，甚至需要从一个页面复制数据到另一个页面，我们通常将这种情况称为**页分裂**。**页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率**。

举个例子，假设某个数据页中的数据是1、3、5、9，且数据页满了，先准备插入一个数据7，则需要把数据页分割为两个数据页。

出现页分裂时，需要将一个页的记录移动到另一个页，性能会受到影响，同时页空间的利用率下降，造成存储空间的浪费。

而如果记录是顺序插入的，例如插入数据11，则只需要开辟新的数据页，也就不会发生页分裂。

因此，在使用InnoDB存储引擎时，如果没有特比的业务需求，建议使用自增字段作为主键。

另外，主键字段的长度不要太大，因为**主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小。**

##### 索引最好设置为NOT NULL

为了更好的利用索引，索引列要设置为NOT NULL约束。有两个原因：

- 第一个原因：索引列存在NULL就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为NULL的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count会省略值为NULL的行。
- 第二个原因：NULL值是一个没有意义的值，但是它会占用物理空间，所以会带来存储空间的问题，因为InnoDB存储记录的时候，如果表中存在允许为NULL的字段，那么行格式中**至少会用1字节空间存储NULL值列表**。

##### 防止索引失效

用上了索引并不意味着查询的时候会使用到索引，所以我们心里要清楚有哪些情况会导致索引失效，从而避免写出索引失效的查询语句，否则这样的查询效率是很低的。

这里简单说一下，发生索引失效的情况：

- 当我们使用左或者左右模糊匹配的时候，也就是`like %xx`或者`like %xx%`这两种方式都会造成索引失效；
- 当我们在查询条件中对索引列做了计算、函数、类型转换操作，这些情况下都会造成索引失效；
- 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
- 在where字句中，如果在or前面的条件列是索引列，而在or后的条件列不是索引列，那么索引会失效。

执行下面这条SQL执行计划：

```mysql
explain select * from t_user where id + 1 = 10;
```

可以知道这条语句没有使用索引，并且是一个全表扫描的查询语句。

对于执行计划，参数有：

- possible_keys字段表示可能用到的索引；
- key字段表示实际用的索引，如果这一项为NULL，说明没有使用索引；
- key_len表示索引的长度；
- rows表示扫描的数据行数；
- type表示数据扫描类型，我们重点看这个。

type字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的**执行效率从低到高的顺序为：**

- ALL（全表扫描）；
- index（全索引扫描）；
- range（索引范围扫描）；
- ref（非唯一索引扫描）；
- eq_ref（唯一索引扫描）；
- const（结果只有一条的主键或唯一索引扫描）。

在这些情况里，all时最坏的情况，因为采用了全表扫描的方式。index和all差不多，只不过index对索引表进行全扫描，这样做的好处时不再需要对数据进行排序，但是开销依然很大。所以，要尽量避免全表扫描和全索引扫描。

range表示采用了索引范围扫描，一般在where字句中使用<、>、in、between等关键词，只检索给定范围的行，属于范围查找。**从这一级别开始，索引的所用会越来越明显，因此我们需要尽量让SQL查询可以使用到range这一级别及以上的type访问方式**。

ref类型表示采用了非唯一索引，或者是唯一索引的非唯一性前缀，返回数据可能是多条。因为虽然使用了索引，但是索引列的值并不唯一，有重复。这样即使使用索引快速查到到了第一条数据，仍然不能停止，要进行目标值附近的小范围扫描。但是它的好处是它并不需要扫全表，因为索引是有序的，即便有重复值，也是在一个非常小的范围内扫描。

eq_ref类型是使用主键或唯一索引时产生的访问方式，通常使用在多表联查中。比如，对两张表进行联查，关联条件时两张表的user_id相等，且user_id是唯一索引，那么使用explain进行执行计划查看的时候，type就会显示eq_ref。

const类型标识使用了主键或者唯一索引与常量值进行比较，比如select name from product where id  = 1。

需要说明的是const类型和eq_ref都使用了主键或唯一索引，不过这两个类型有所区别，**const是与常量进行比较，查询效率会更快，而eq_ref通常用于多表联查中**。



**除了关注type，我们也要关注extra显示的结果。**

这里说几个重要的参考指标：

- Using filesort：当查询语句中包含group by操作，而且无法利用索引完成排序操作的时候，这时不得不选择相应的排序算法进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。
- Using temporary：使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表，常见于排序order by和分组查询group by。效率低，要避免这种问题的出现。
- Using index：所需数据只需要在所有即可全部获得，不需要再到表中取数据，也就是使用了覆盖索引，避免了回表操作，效率不错。

#### 总结

本节主要介绍了索引的原理、分类和使用

**为什么MySQL InnoDB选择B+Tree作为索引的数据结构？**

- B+Tree vs B Tree：-
  - 存储相同数据量级别的情况下，B+Tree高比B Tree低，磁盘I/O次数更少。
  - B+Tree叶子节点用双向链表串起来，适合范围查询，B Tree无法做到这点。
- B+Tree vs 二叉树：
  - 随着数据量的增加，二叉树的树高会越来越高，磁盘I/O次数也会更多，B+Tree在千万级别的数据量下，高度依然维持在3~4层左右，也就是说一次数据查询操作只需要做3~4次磁盘I/O操作就能查询到目标数据。
- B+Tree vs Hash：
  - 虽然Hash等值查询的效率很高，但是无法做范围查询

**什么时候适用索引？**

- 字段有唯一限制的，比如商品编码；
- 经常用与where查询条件的字段；
- 经常用于group by 和 order by 的字段；

**什么时候不需要创建索引？**

- where条件，group by，order by 里用不到的字段；
- 字段中存在大量重复数据，不需要创建索引；
- 表数据太少的时候，不需要创建索引；
- 经常更新的字段不用创建索引；

**什么时候索引会失效？**

- 当我们适用做或者左右模糊匹配的时候，也就是`like %xx`或者`like %xx%`这两种方式都会造成索引失效；
- 当我们在查询条件中对索引做了计算、函数、类型转换操作，都会导致索引失效；
- 联合索引要能正确适用需遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
- 在where子句中，如果在or前的条件列是索引列，而在or后的条件列不是索引列，那么索引会失效；
- 为了更好的利用索引，索引列要设置为NOT NULL约束。

**有什么优化索引的方法？**

- 前缀索引优化；
- 覆盖索引优化；
- 主键索引最好是自增的；
- 防止索引失效。

### 从数据页的角度看B+树

大家背八股文的时候，都知道MySQL里InnoDB存储引擎是采用B+树来组织数据的。

这点没错，但是大家知道B+树的节点里方的是什么呢？查询数据的过程又是怎样的？

这次，我们**从数据页的角度看B+树**，看看每个节点长啥样。

#### InnoDB是如何存储数据的？

MySQL支持多种存储引擎，不同的存储引擎，存储数据的方式也是不同的，我们最常使用的是InnoDB存储引擎，索引就跟大家讲解一下InnoDB是如何存储数据的。

记录是按照行来存储的，但是数据库的读取并不以`行`为单位，否则一次读取（也就是一次I/O操作）只能处理一行数据，效率会非常低。

因此，**InnoDB的数据是按`页数据`为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。

数据库的I/O操作的最小单位是页，**InnoDB数据页默认大小是16KB**，意味着数据库每次读写都是以16KB为单位的，一次最少从磁盘读取16K的内容到内存中，一次最少吧内存中的16K内容刷新到磁盘中。

数据页包括七个部分：

| 名称                            | 说明                                               |
| ------------------------------- | -------------------------------------------------- |
| 文件头 File Header              | 文件头，表示页的信息                               |
| 页头 Page Header                | 页头，表示页的状态信息                             |
| 最小和最大记录 Infimum+Supermum | 两个虚拟的伪记录，分别表示页中的最小记录和最大记录 |
| 用户记录 User Records           | 存储行记录内容                                     |
| 空闲空间 Free Space             | 页中还没被使用的空间                               |
| 页目录 Page Directory           | 存储用户记录的相对位置，对记录起到索引作用         |
| 文件尾 File Tailer              | 校验页是否完整                                     |

在File Header中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向链表。

采用链表的结构是让数据页之间不需要是物理上是连续的，而是逻辑上的连续。

**数据页中的记录按照`主键`顺序组成单向链表**，单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。

因此，数据页中有一个**页目录**，起到记录的索引作用，就像我们书那样，针对书中内容的每个章节设立了一个目录，想看某个章节的时候，可以查看目录，快速找到对应的章节的页数，而数据页中的页目录就是为了快速找到记录。

那么InnoDB是如何给记录创建页目录的呢？

页目录创建的过程如下：

1. 将所有的记录划分成几个组，这些记录包括最小记录和最大记录，但不包括标记为“已删除”的记录；
2. 每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为n_owned字段
3. 页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**。

**页目录就是由多个槽组成的，槽相当于分组记录的索引。**然后，因为记录时按照`主键值`从小到大排序的，所以**我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录，**无需从最小记录开始遍历整个页中的记录链表。

举个例子，5个槽的编号分别为0，1，2，3，4，我想查找主键为11的用户记录：

- 先二分得出槽的中间位时（0+4）/2=2，2号槽里最大的记录为8。因为11>8，所以需要从2号槽后继续搜索记录；
- 再使用二分搜索出2号槽和4号槽的中间位时（2+4）/2=3，3号槽里最大的记录为12。因为11<12，所以主键为11的记录再3号槽里；
- 这里有个问题，`槽对应的值都是这个组的主键最大的记录，如何找到组内最小的记录`？比如槽3对应最大主键是12的记录，那么如何找到最小记录9。解决方法是：通过槽3找到槽2对应的记录，也就是主键为8的记录。主键为8的记录的下一记录就是槽3当中主键最小的9记录，然后开始向下搜索2次，定位到主键为11的记录，取出该条记录的信息即为我们想要查找的内容。

看到第三步的时候，可能有的同学会疑问，如果某个槽内的记录很多，然后因为记录都是单向链表串起来的，那么这样在槽内查找某个记录的时间复杂度不就是O（n）了吗？

这点不用担心，InnoDB对每个分组中的记录条数都是有规定的，槽内的记录就只有几条：

- 第一个分组中的记录只能有1条记录；
- 最后一个分组中的记录条数范围只能在1~8条之间；
- 剩下的分组中的记录条数范围只能在4~8条之间。

#### B+树是如何进行查询的？

上面我们都是在说一个数据页中的记录检索，因为一个数据页中的记录是有限的，且主键值是有序的，所以通过对所有记录进行分组，然后将组号（槽号）存储到页目录，使其起到索引作用，通过二分查找的方式快速检索到记录在哪个分组，来降低检索的时间复杂度。

但是，当我们需要存储大量的记录时，就需要多个数据页，这时我们就需要考虑如何建立合适的索引，才能方便定位记录所在页。

为了解决这个问题，**InnoDB采用了B+树作为索引**。磁盘的I/O操作次数对索引的使用效率至关重要，因此在构造索引的时候，我们更倾向于采用“矮胖”的B+树数据结构，这样所需要进行的磁盘I/O次数更少，而且B+树更适合进行关键字的范围查询。

InnoDB里的B+树中的**每个节点都是一个数据页**：

- 只有叶子节点（最底层的节点）才存放了数据，非叶子节点（其他上层节点）仅用来存放目录项作为索引。
- 非叶子节点分为不同层次，通过分层来降低每一层的搜索量；
- 所有节点按照索引键大小排序，构成一个双向链表，便于范围查询；

我们再看看B+树如何实现快速查找主键为6的记录：

- 从根节点开始，通过二分查找快速定位到符合页内范围包含查询值的页，因为查询的主键值为6，在[1, 7 )范围之间，所以到页30中查找更详细的目录项；
- 在非叶子节点（页30）中，继续通过二分法快速定位到符合页内范围包含查询值得页，主键值大于5，所以就到叶子节点（页16）查找记录；
- 接着，在叶子节点（页16）中，通过槽查找记录时，使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽之后，再遍历槽内的所有记录，找到主键为6的记录。

可以看到，在定位记录在哪一个页时，也是通过二分法快速定位到包含该记录的页。定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号），最后在分组内进行遍历查找。

#### 聚簇索引和二级索引

另外，索引又可以分为聚簇索引和非聚簇索引（二级索引），它们的区别就在于叶子节点存放的适合什么数据：

- 聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点；
- 二级索引的叶子节点存放的是主键值，而不是实际数据。

因为表的数据都是存放在聚簇索引的叶子节点里，所以InnoDB存储引擎一定会为表创建一个聚簇索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个。

InnoDB在创建聚簇索引时，会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键；
- 如果没有主键，就选择第一个不包含NULL值得唯一列作为聚簇索引的索引键；
- 在上面两个都没有的情况下，InnoDB将自动生成一个隐式id列作为聚簇索引的索引键；

一张表只能有一个聚簇索引，那为了实现非主键字段的快速搜索，就引出了二级索引（非聚簇索引/辅助索引），它也是利用了B+树的数据结构，但是二级索引的叶子节点存放的是主键值，不是实际数据。

因此，**如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获取数据行，这个过程就叫做`回表 `，也就是说要查两个B+树才能查到数据。不过，当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作`覆盖索引`，也就是只需要查一个B+树就能找到数据。**

#### 总结

InnoDB的数据时是`数据页`为单位来读写的，默认数据页大小为16KB。每个数据页之间通过双向链表的形式组织起来，物理上不连续，但是逻辑上连续。

数据页内包含用户记录，每个记录之间用单向链表的方式组织起来，为了加快在数据页内高效查询记录，设计了一个页目录，页目录存储各个槽（分组），且主键值是有序的，于是可以通过二分查找法的方式进行检索从而提高效率。

为了高效查询记录所在的数据页，InnoDB采用B+树作为索引，每个节点都是一个数据页。

如果叶子节点存储的是实际数据就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。

在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是`索引覆盖`操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获取数据行，这个过程就叫做`回表`。

### 为什么MySQL采用B+树作为索引？

`为什么MySQL采用B+树作为索引？`这句话，是不是在面试里经常出现。

要解释这个问题，其实不单单要从数据结构的角度出发，还要考虑磁盘I/O操作次数，因为MySQL的数据时存储在磁盘中的。

#### 怎么的索引的数据结构是好的？

MySQL的数据是持久化的，意味着数据（索引+记录）是保存到磁盘上的，因为这样即使设备断电了，数据也不会丢失。

磁盘是一个慢的离谱的存储设备，有多离谱呢？

人家内存的访问的速度是纳秒级别的，而磁盘访问的速度是毫秒级别的，也就是说读取同样大小的数据，从磁盘中读取的速度比从内存中读取的速度要慢上万倍，甚至几十万倍。

磁盘读写的最小单位是**扇区**，扇区的大小只有`512B`大小，操作系统一次会读写多个扇区，所以**操作系统的最小读写单位是块（Block）。Linux中的块大小为`4KB`**，也就是一次磁盘I/O操作会直接读写8个扇区。

由于数据库的索引是保存到磁盘上的，因此当我们通过索引查找某行数据的时候，就需要先从磁盘读取索引到内存，再通过索引从磁盘中找到某行数据，然后读入到内存，也就是说查询过程中会发生多次磁盘I/O，而磁盘I/O次数越多，所消耗的时间也就越大。

所以，我们希望索引的数据结构能在尽可能少的磁盘I/O操作中完成查询工作，因为磁盘I/O操作越少，所消耗的时间也就越小。

另外，MySQL是支持范围查找的，所以索引的数据结构不仅要能高效地查询某一个记录，而且也要能高效地执行范围查找。

所以，要设计一个适合MySQL索引地数据结构，至少满足以下要求：

- 能在尽可能少地磁盘I/O操作中完成查询工作；
- 要能高效地查询某一个记录，也要能高效地执行范围查找；

分析完要求后，我们针对每一个数据结构分析以下。

#### 什么是二分查找？

索引数据最好能按顺序排列，这样可以使用`二分查找法`高效定位数据。

假设我们现在用数组来存储索引，比如有一个排序的数组，如果要从中找出数字3，最简单的方法就是从头依次遍历查询，这种方法的时间复杂度是O（n），查询效率并不高。因为该数组是有序的，所以我们可以采用二分查找法。

可以知道，二分查找法每次都把查询的范围减半，这样时间复杂度就降到了O（logn），但是每次查找都需要不断计算中间位置。

#### 什么是二分查找树？

用数组来实现线性排序的数据虽然简单好用，但是插入新元素的时候性能太低。

因为插入一个元素，需要将这个元素之后的所有元素后移一位，如果这个操作发生在磁盘中呢？这必然是灾难性的。因此磁盘的速度比内存慢几十万倍，所以我们不能用一种线性结构将磁盘排序。

其次，有序的数组在使用二分查找的时候，每次查找都要不断计算中间的位置。

那我们能不能设计一个非线性且天然适合二分查找的数据结构呢？

有的，找到所有二分查找中用到的所有中间节点，把他们用指针连起来，并将中间节点作为根节点。

怎么样？是不是变成了二叉树，不过它不是普通的二叉树，它是一个二叉查找树。

**二叉查找树的特点是一个节点的左子树的所有节点都会小于这个节点，右子树的所有节点都会大于这个节点**，这样我们在查询数据时，不需要计算中间节点的位置了，只需要将查询的数据与节点的数据进行比较。

假设，我们查找索引值为key的节点：

1. 如果key大于根节点，则在右子树中进行查找；
2. 如果key小于根节点，则在左子树中进行查找；
3. 如果key等于根节点，也就是找到了这个节点，返回根节点即可。

另外，二叉查找树解决了插入新节点的问题，因为二叉查找树是一个跳跃结构，不必连续排列。这样在插入的时候，新节点可以放在任何位置，不会像线性结构那样插入一个元素，所有元素都需要向后排列。

因此，二叉查找树解决了连续结构插入新元素开销很大的问题，同时又保持着天然的二分结构。

那是不是二叉查找树就可以作为索引的数据结构了呢？

不行，二叉查找树存在一个极端情况，会导致它变成一个瘸子！

**每次插入的元素都是二叉查找树中最大的元素，二叉查找树就会退化成一条链表，查找数据的时间复杂度会变成O（n）。**

由于树时存储在磁盘中的，访问每个节点，都对应一次磁盘I/O操作（假设一个节点的大小`小于`操作系统的最小读写单位快的大小），也就是说**树的高度等于每次查询数据时磁盘I/O操作的次数**，所以树的高度越高，就会影响查询性能。

二叉查找树由于存在退化成链表的可能性，会使得查询操作的时间复杂度从O(logn)升为O(n)。

而且会随着插入的元素越多，树的高度也变高，意味着需要磁盘I/O操作的次数就越多，这样导致查询性能严重下降，再加上不能范围查询，所以不适合作为数据库的索引结构。

#### 什么是自平衡二叉树？

为了解决二叉查找树会在极端情况下退化成链表的问题，后面就有人提出**平衡二叉查找树（AVL树）**。

主要是在二叉查找树的基础上增加了一些条件约束：**每个节点的左子树和右子树的高度差不能超过1**。也就是说节点的左子树和右子树仍然为平衡二叉树，这样查询操作的时间复杂度就会一直维持在O(logn)。

除了平衡二叉查找树，还有很多自平衡的二叉树，比如红黑树，它也是通过一些约束条件来达到自平衡，不过红黑树的约束条件比较复杂，这里不做赘述。

**不管平衡二叉查找树还是红黑树，都会随着插入的元素增多，而导致树的高度变高，这就意味着磁盘I/O操作次数多，会影响整体数据查询的效率。**

**当树的节点越多的时候，并且树的分叉数M越大的时候，M叉树的高度会远小于二叉树的高度。**

#### 什么是B树

自平衡二叉树虽然能保持查询操作的时间复杂度在O(logn)，但是因为它本质上是一个二叉树，每个节点只能有2个子节点，那么当节点个数越多的时候，树的高度也会相应变高，这样就会增加磁盘的I/O次数，从而影响查询的效率。

为了降低树的高度问题，后面就出来了B树，它不再限制一个节点只能有2个子节点，而是允许M个子节点（M>2），从而降低树的高度。

B树的每一个节点最多可以包括M个子节点，M称为B树的阶，所以B树就是一个多叉树。

假设M=3，那么就是一个3阶的B树，特点就是每个节点最多有2个（M-1）数据和最多有3个（M个）子节点，超过这些要求的话，就会分裂节点。

假设我们在一颗3阶的B树中要查找的索引值是9的记录，那么步骤可以分为以下几步：

1. 与根节点的索引（4，8）进行比较，9大于8，纳闷往右边的子节点走；
2. 然后该子节点的索引为（10，12），因为9小于10，所以会往该节点的左边子节点走；
3. 走到索引为9的节点，然后我们找到了索引值9的节点。

可以看到，一颗3阶的B树在查询叶子节点中的数据时，由于树的高度是3，所以在查询过程中会发生3次磁盘I/O操作。

而如果同样的节点数量在平衡二叉树的场景下，树的高度就会很高，意味着磁盘I/O操作会更多。所以，B树在数据查询中比平衡二叉树效率要高。

但是B树的每个节点都包含数据（索引和记录），而用户的记录数据大小很有可能远远超过了索引数据，这就需要花费更多的磁盘I/O操作次数来读到`有用的索引数据`。

而且，在我们查询位于底层的某个节点（比如A记录）过程中，`非A记录节点`里的记录数据会从磁盘加载到内存，但是这些记录数据是没用的，我们只是想读取这些节点的索引数据来做比较查询，而`非A记录节点`里的记录数据对我们是没用的，这样不仅增多磁盘I/O操作次数，也占用内存资源。

另外，如果使用B树来做范围查询的话，需要使用中序遍历，这回涉及到多个节点的磁盘I/O问题，从而导致整体速度下降。

#### 什么是B+树？

B+树就是对B树做了一个升级，MySQL中索引的数据结构就是采用了B+树。

B+树与B树差异的点，主要是以下几点：

- 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
- 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；
- 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）；
- 非叶子节点中有多少个子节点，就有多少个索引；

下面通过三个方面，比较下B+树和B树的性能区别。

##### 1、单点查询

B树进行单个索引查询时，最快可以在O(1)的时间代价内就查到，而从平均时间代价来看，会比B+树稍块一些。

但是B树的查询波动会比较大，因为每个节点既存索引又存记录，所以有时候访问到了非叶子节点就可以找到索引，而有时需要访问到叶子节点才能找到索引。

**B+树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储既存索引又存记录的B树，B+树的非叶子节点可以存放更多的索引，因此B+树可以比B树更`矮胖`，查询底层节点的磁盘I/O次数会更少。**

##### 2、插入和删除效率

B+树有大量的冗余节点，这样使得删除一个节点的时候，可以直接从叶子节点中删除，甚至可以不动非叶子节点，这样删除非常快。

**注意：B+树对于非叶子节点的节点数和索引的个数，定义方式可能会不同，有的说非叶子节点的节点数的个数是M阶，而索引的个数为M-1（这个是维基百科的定义）。但是前面介绍B树和B+树的差异时，说的是`非叶子节点中有多少个子节点，就有多少个索引`，主要是MySQL用到的B+树就是这个特性。**

B树删除节点的过程，可能会导致树的复杂变化。但是，B+树在删除根节点的时候，由于存在冗余的节点，所以不会发生复杂的树变形。

B树则不同，B树没有冗余节点，删除节点的时候非常复杂，比如删除根节点中的数据，可能涉及复杂的树变形。

B+树的插入也是一样，有冗余节点，插入可能存在节点的分裂（如果节点饱和），但是最多只涉及树的一条路径。而且B+树会自动平衡，不需要像更多复杂的算法，类似红黑树的选择操作等。

因此，**B+树的插入和删除效率更高。**

##### 3、范围查询

B树和B+树等值查询原理基本一致，先从根节点查找，然后对比目标数据的范围，最后递归进入子节点查找。

因为**B+树所有叶子节点间还有一个链表进行连接，这种设计对范围查找非常有帮助**，比如说我们想知道12月1日和12月12日之间的订单，这个时候可以先查找到12月1日所在的叶子节点，然后利用链表向右遍历，知道找到12月12日的节点，这样就不需要从根节点查询了，进一步节省查询需要的时间。

而B树没有将所有叶子节点用链表串联起来的结构，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘I/O操作，范围查询效率不如B+树。

因此，存在大量范围检索的场景，适合使用B+树，比如数据库。而对于大量的当个索引查询的场景，可以考虑B树，比如nosql的MongoDB。

##### MySQL中的B+树

MySQL的存储方式根据存储引擎的不同而不同，我们最常用的就是InnoDB存储引擎，它就是采用了B+树作为索引的数据结构。

但是InnoDB使用的B+树有一些特别的点，比如：

- B+树的叶子节点之间是用`双向链表`进行连接，这样的好处是既能向右遍历，也能向左遍历。
- B+树节点内容是数据页，数据页存放了用户的记录以及各种信息，每个数据页默认大小是16KB。

InnoDB根据索引类型不同，分为聚簇和二级索引。它们区别在于，聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚集索引的叶子节点，而二级索引的叶子节点存放的是主键值，而不是实际数据。

因为表的数据都是存放在聚簇索引的叶子节点里，所以InnoDB存储引擎一定会为表创建一个聚簇索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个，而二级索引可以创建多个。

#### 总结

MySQL是会将数据持久化在硬盘，而存储功能是由MySQL存储引擎实现的，所以讨论MySQL使用哪种数据结构作为索引，实际上是在讨论存储引擎使用哪种数据结构作为索引，InnoDB是MySQL默认的存储引擎，它就是采用B+树作为索引的数据结构。

要设计一个MySQL的索引数据结构，不仅仅考虑数据结构增删改的时间复杂度，更重要的是要考虑磁盘I/O的操作次数。因为索引和记录都是存放在硬盘，硬盘是一个非常慢的存储设备，我们在查询数据的时候，最好能在尽可能少的磁盘I/O的操作次数内完成。

二分查找树虽然是一个天然的二分结构，能很好的利用二分查找快速定位数据，但是它存在一种极端的情况，每当插入的元素都是树内最大的元素，就会导致二分查找树退化成一个链表，此时查询复杂度就会从O(logn)降低为O(n)。

为了解决二分查找树退化成链表的问题，就出现了自平衡二叉树，保证了查询操作的时间复杂度会一直维持在O(logn)。但是它的本质还是一个二叉树，每个节点只能有2个子节点，随着元素的增多，树的高度会越来越高。

而树的高度会决定磁盘I/O操作的次数，因为树是存储在磁盘中的，访问每个节点，都对应一次磁盘I/O操作，也就是说树的高度就等于每次查询数据时磁盘I/O操作的次数，所以树的高度越高，就越会影响查询性能。

B树和B+树都是通过多叉树的方式，将树的高度变矮，所以这两个数据结构非常适合检索存于磁盘中的数据。

但是MySQL默认的存储引擎InnoDB采用的是B+树作为索引的数据结构，原因有：

- B+树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储既存索引又存记录的B树，B+树的非叶子节点可以存放更多的索引，因此B+树可以比B树更`矮胖`，查询底层节点的磁盘I/O次数会更少。
- B+树又大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让B+树在插入、删除的效率都更高，比如删除根节点的适合，不会像B树那样会发生复杂的树变化；
- B+树叶子节点之间用链表连接起来，有利于范围查询，而B树要实现范围查询，因此只能通过树的遍历来完成范围查询，这回涉及多个节点的磁盘I/O操作，范围查询效率不如B+树。

### MySQL单表不要超过2000W行，靠谱吗？

作为在后端圈开车多年的老司机，是不是经常听到过：

- MySQL单表最好不要超过2000W
- 单表超过2000W就要考虑数据迁移了
- 你这个表数据都马上要到2000W了，难怪查询速度慢

#### 实验

实验一把看看，建一张表

```mysql
CREATE TABLE person(
    id int NOT NULL AUTO_INCREMENT PRIMARY KEY comment '主键',
    person_id tinyint not null comment '用户id',
    person_name VARCHAR(200) comment '用户名称',
    gmt_create datetime comment '创建时间',
    gmt_modified datetime comment '修改时间'
) comment '人员信息表';
```

插入一条数据

```mysql
insert into person values(1, 1,'user_1', now(), now());
```

利用 MySQL 伪列 rownum 设置伪列起始点为 1

```mysql
select (@i:=@i+1) as rownum, person_name from person, (select @i:=100) as init; 
set @i=1;
```

运行下面的sql，连续执行20次，就是2的20次方 约等于100W的数据；执行23次就是2的23次方约等于800W，如此下去即可实现千万测试数据的插入。

如果不想翻倍增加数据，而是想很少量，有个技巧，就是在SQL的后面增加where条件，如id>某一个值去控制增加的数据量即可。

```mysql
insert into person(id, person_id, person_name, gmt_create, gmt_modified)
select @i:=@i+1,
left(rand()*10,10) as person_id,
concat('user_',@i%2048),
date_add(gmt_create,interval + @i*cast(rand()*100 as signed) SECOND),
date_add(date_add(gmt_modified,interval +@i*cast(rand()*100 as signed) SECOND), interval + cast(rand()*1000000 as signed) SECOND)
from person;
```

此处需要注意的是，也许你在执行到近800W或者1000W数据的时候，会报错：The total number of locks exceeds the lock table size。

这是由于你的临时表内存设置的不够大，只需要扩大以下设置参数即可。

```mysql
SET GLOBAL tmp_table_size =512*1024*1024; （512M）
SET global innodb_buffer_pool_size= 1*1024*1024*1024 (1G);
```

然后测试以下查询耗时，测试结果似乎和标题真的对应了，当数据达到2000W以后，查询时长急剧上升，难道这就是铁律吗？

那下面我们就来看看这个建议值2000W是怎么来的？

#### 单表数量限制

首先我们先想想数据库单表行数最大多大？

```mysql
CREATE TABLE person(
    id int(10) NOT NULL AUTO_INCREMENT PRIMARY KEY comment '主键',
    person_id tinyint not null comment '用户id',
    person_name VARCHAR(200) comment '用户名称',
    gmt_create datetime comment '创建时间',
    gmt_modified datetime comment '修改时间'
) comment '人员信息表';
```

看看上面的建表sql。id是主键，本身就是唯一的，也就是说主键的大小可以限制表的上线：

- 如果主键声明`int`类型，也就是32位，那么支持2^32-1约等于21亿；
- 如果主键声明`bigint`类型，那就是2^62-1（36893488147419103232），难以想象这个有多大了，一般还没有到这个限制之前，可能数据库已经爆满了！

有人统计过，如果建表的时候，自增字段选择无符号的bigint，那么自增最大值是18446744073709551615，按照一秒新增一条记录的速度，大约什么时候能用完？

| 一秒增加的记录数 | 大约多少年用完 |
| ---------------- | -------------- |
| 1/1秒            | 584942417355年 |
| 1W/秒            | 58494241年     |
| 100W/秒          | 584942年       |
| 1亿/秒           | 5849年         |

#### 表空间

以person表为例，表数据实际上是放在一个叫person.ibd（innodb data）的文件中，也叫做表空间；虽然数据表中，它们看起来是一条连着一条，但实际上在文件中它倍分成很多小份的数据页，而且每一份都是16K。

#### 页得数据结构

一个InnoDB数据页得存储空间大致被分为7个部分，有的部分占用的字节数是确定的，有的部分占用的字节数是不确定的。

在页的7个组成部分中，我们自己存储的记录会按照我们指定的行格式存储到`User Records`部分。

但是在一开始生成页的时候，其实并没有User Records这个部分，每当我们插入一条记录，都会从Free Space部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到User Records部分。

当Free Space部分的空间全部被User Records部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了。

刚刚上面说到了数据的新增过程。

那下面就来说说，数据的查找过程，假如我们需要查找一条记录，我们可以把表空间中的每一页都加载到内存中，然后对记录挨个判断是不是我们想要的。

在数据量小的时候，没什么问题，内存也可以撑。但是现实就是这么残酷，不会给你这个局面。

为了解决这个问题，MySQL中就有了索引的概念，大家都知道索引能够加快数据的查询，那到底是怎么个回事呢？

#### 索引的数据结构

在MySQL中索引的数据结构和刚刚描述的页几乎是一模一样的，而且大小也是16K。

但是在索引页中记录的页（数据页，索引页）的最小主键id和页号，以及在索引页中新增了层级的信息，从0开始往上算，所以页与页之间就有了上下层级的概念。

从单个节点来看，首先它是一个非叶子节点（索引页），在它的内容区中有id和页号地址两部分：

- id：对于页中记录的最小记录id值；
- 页号：地址是指向对应页的指针；

而数据页与此几乎大同小异，区别在于数据页记录的是真实地行数据而不是页地址，而id也是顺序的。

#### 单表建议值

下面我们就以3层，2分叉（实际中是M分叉）来说明以下查找一个行数据的过程。

比如说我们需要查找一个id=6的行数据：

- 因为在非叶子节点中存放的是页号和该页最小的id，所以我们从顶层开始对比，首先看页号10中的目录，有[id=1，页号=20]，[id=5，页号=30]，说明左侧节点最小id为1，右侧节点最小id是5。6>5，那按照二分法查找的规则，肯定就往右侧节点继续查找；
- 找到页号30的节点后，发现这个节点还有子节点（非叶子节点），那就继续对比，同理，6>5&6<7，所以找到了页号60；
- 找到页号60之后，发现此节点为叶子节点（数据节点），于是将次页数据加载到内存进行一一对比，结果找到了id=6的数据行。

从上述过程中发现，我们为了查找id=6的数据，总共查询了三个页，如果这三个页都在磁盘中（未提前加载到内存），那么最多需要经历三次的磁盘I/O。

需要注意的是，例子中的页号只是个示例，实际情况并不是连续的，在磁盘中存储也不一定是顺序的。

至此，我们大概已经了解了表的数据是怎么个结构了，也大概知道查询数据是个怎么的过程了，这样我们也就能够大概估算这样的结构能存放多少数据了。

我们可以知道B+树的叶子节点才是存数据的，而非叶子节点是用来存放索引数据的。

所以，同样是16K的页，非叶子节点里的每条数据都指向新的页，而新的页有两种可能

- 如果是叶子节点，那么里面就是一行行的数据
- 如果是非叶子节点的话，那么就会继续指向新的页

假设

- 非叶子节点内指向其他页的数量为x
- 叶子节点内能容纳的数据行树为y
- B+树的层数为z

最终total=x^(z-1)*y，也就是说总数会等于x的z-1次方与y的乘积。

**X=？**

在文章的而开头已经介绍了页的结构，索引也不例外，都会有File Header（38byte）、Page Header（56 byte）、Infimum+Supermum（26 byte）、File Trailer（8 byte），再加上页目录，打开1k左右。

我们就把它当作是1K，整个页的大小是16K，剩下的15K用于存数据，在索引页中主要记录的是主键与页号，主键我们假设是Bigint（8 byte），而页号也是固定的（4 byte），那么索引页中的一条数据也就是12 byte。

所以x=15*1024/12≈1280行

**Y=？**

叶子节点和非叶子节点的结构是一样的，同理，能放数据的空间也是15k。

但是叶子节点中存放的是真正的行数据，这个影响的因素就会很多很多。比如，字段的类型，字段的数量。每行数据占用的空间越大，页中所放的行数量就会越少。

这边我们暂时按一条行数据1K来算，那一页就能存下15条，Y=15*1024/1000≈15。

算到这边了，是不是心里已经有谱了啊。

根据上述的公式，total=x^(z-1)*y，已知x=1280，y=15：

- 假设B+树是两层，那就是z=2，total=(1280^1)*15=19200
- 假设B+树是三层，那就是z=3，total=(1280^2)*15=24576000（约2.45kw）

这不正好就是文章开头说的最大行数建议值2000W吗？对的，一般B+树的层级最多也就是三层。

试想一下，如果是4层，除了查询的时候磁盘I/O次数会增加，而且这个total值会是多少，大概应该是三百多亿吧，也不太合理，所以，三层应该是比较合理的一个值。

**到这里难道就完了？**

不。

我们刚刚在说Y值得时候假设是1K，那比如我实际单行数据占用空间不是1K，而是5K，那么单个数据页最多只能放下3条数据。

同样，还是按照z=3的值来计算，那么total=1280^2)*3=4915200（近500W）

所以，在保持相同层级（相似查询性能）的情况下，在行数据大小不同的情况下，其实这个最大建议值也是不同的，而且影响查询性能的还有很多其他因素，比如，数据库版本，服务器配置，sql的编写等。

MySQL为了提高性能，会将表的索引装载到内存中，在InnoDB buffer size足够的情况下，其能完全加载进内存，查询不会有问题。

但是，当单表数据库到达某个量级上限的时候，导致内存无法存储其索引，使得之后的SQL查询会产生磁盘I/O，从而导致性能下降，所以增加硬件配置（比如把内存当磁盘使），可能会带来立竿见影的性能提升。

#### 总结

- MySQL的表数据是以页的形式存放的，页在磁盘中不一定是连续的。
- 页的空间是16K，并不是所有的空间都是用来存放数据的，会有一些固定的信息，如页头，页尾，页码，校验码等等。
- 在B+树中，叶子节点和非叶子节点的数据结构是一样的，区别在于，叶子节点存放的是实际的行数据，而非叶子节点存放的是主键和页号。
- 索引结构不会影响单表最大行数，2000W也只是推荐值，超过了这个值可能会导致B+树层级更高，影响查询性能。

### 索引失效有哪些？

#### 索引存储结构长什么样？

我们先来看看索引存储结构长什么样？因为只有知道索引的存储结构，才能更好的理解索引失效的问题。

索引的存储结构跟MySQL使用哪种存储引擎有关，因为存储引擎就是负责将数据持久化在磁盘中，而不同的存储引擎采用的索引数据结构也会不相同。

MySQL默认的存储引擎是InnoDB，它采用B+Tree作为索引的数据结构。

在创建表时，InnoDB存储引擎会默认创建一个主键索引，也就是聚簇索引，其他所有都属于二级索引。

MySQL的MyISAM存储引擎支持多种索引数据结构，比如B+树索引，R树索引、Full-Text索引。

MyISAM存储引擎在创建表时，创建的主键索引默认使用的是B+树索引。

虽然，InnoDB和MyISAM都支持B+树索引，但是它们数据的存储结构方式不同。不同之处在于：

- InnoDB存储引擎：B+树索引的叶子节点保存数据本身；
- MyISAM存储引擎：B+树索引的叶子节点保存数据的物理地址；

InnoDB存储引擎根据索引类型不同，分为聚簇索引和二级索引。它们区别在于，聚簇索引的叶子节点存放的是实际数据，所有完整的用户数据都存放在聚簇索引的叶子节点，而二级索引的叶子节点存放的是主键值，而不是实际数据。

知道了InnoDB存储引擎的聚簇索引和二级索引的存储结构后，接下来举几个查询语句，说一下查询过程是怎么选用那些索引类型的。

在我们使用`主键索引`字段作为条件查询的时候，如果要查询的数据都在`聚簇索引`的叶子节点里，那么就会在`聚簇索引`中的B+树检索到对应的叶子节点，然后直接读取要查询的数据。如下面这条语句：

```mysql
select * from t_user where id = 1;
```

在我们使用`二级索引`字段作为查询条件的时候，如果要查询的数据都在`聚簇索引`的叶子节点里，那么需要检索两颗B+树：

- 先在`二级索引`的B+树找到对应的叶子节点，获取主键值；
- 然后根据上一步获取的主键值，在`聚簇索引`中的B+树检索到对应的叶子节点，然后获取要查询的数据。

上面这个过程叫做回表，如下面这条语句：

```mysql
select * from t_user where name="xxx";
```

在我们使用`二级索引`字段作为查询条件的时候，如果要查询的数据在`二级节点的叶子节点`，那么只需要在`二级索引`的B+树中找到对应的叶子节点，然后读取要查询的数据，这个过程叫做**覆盖索引**。如下面这条语句：

````mysql
select id form t_user where name="***";
````

上面这些查询语句的条件都用到了索引列，所以在查询过程都用上了索引。

但是并不意味着，查询条件用上了索引列，就查询过程就一定用上了索引，接下来我们再一起看看那些情况会导致索引失效，而发生全表扫描。

首先说明下，下面的实验案例，使用的MySQL版本为`8.0.26`

#### 对索引使用左或左右模糊匹配

当我们使用左或者左右模糊匹配的时候，也就是`like %xx`或者`like %xx%`这两种方式都会造成索引失效。

比如说下面的like语句，查询name后缀为`林`的用户，执行计划中的type=ALL就代表了全表扫描，而没有走索引。

```mysql
select * from t_user where name like '%林';
```

如果是查询name前缀为`林`的用户，那么就会走索引扫描，执行计划中的type=range表示走索引扫描，key=index_name看到实际走了index_name索引：

```mysql
select * from t_user where name like '林%';
```

**为什么like关键字左或者左右模糊匹配无法走索引呢？**

**因为索引B+树是按照`索引值`有序排列存储的，只能根据前缀进行比较。**

举个例子，假设我们要查询name字段前缀为`林`的数据，也就是`name like '林%'`，扫描索引的过程：

- 首节点查询比较：林这个字的拼音大小比首节点的第一个索引值的陈字大，但是比首节点的第二个索引值的周字小，所以选择去节点2继续查询；
- 节点2查询比较：节点2的第一个索引值中的陈字的拼音大小比林字小，所以继续看下一个索引值，发现节点2有与林字前缀匹配的索引值，于是就往叶子节点查询，即叶子节点4；
- 节点4查询比较：节点4的第一个索引值的前缀符合林字，于是就读取该行数据，接着继续往右匹配，直到匹配不到前缀为林的索引值。

如果使用`name like '%林'`方式来查询，因为查询的结果可能是`陈林、张林、周林`等之类，所以不知道从哪个索引值开始比较，于是就只能通过全表扫描的方式来查询。

#### 对索引使用函数

有时候我们会用一些MySQL自带的函数来得到我们想要的结果，这时候要注意了，如果查询条件中对索引字段使用函数，就会导致索引失效。

比如下面这条语句查询条件中对name字段使用了LENGTH函数，执行计划中的type=ALL，代表了全表扫描：

```mysql
select * from t_user where length(name)=6;
```

**为什么对索引使用函数，就无法走索引了呢？**

因为索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了。

不过，从MySQL 8.0开始，索引特性增加了函数索引，可以针对函数计算后的值建立一个索引，也就是说该索引的值是函数计算后的值，所以就可以通过扫描索引来查询数据。

举个例子，通过下面这条语句，对length(name)的计算结果建立一个名为idx_name_length的索引

```mysql
alter table t_user add key idx_name_length ((length(name)));
```

然后我再用下面这条查询语句，这时候就会走索引了。

#### 对索引进行表达式运算

在查询条件中对索引进行表达式计算，也是无法走索引的。

比如，下面这条查询语句，执行计划中type=ALL，说明是通过全表扫描的方式查询数据的：

```mysql
explain select * from t_user where id + 1 = 10;
```

但是，如果把查询语句的条件改成where id = 10 - 1，这不就是在索引字段上进行表达式计算了，于是就可以走索引查询了。

**为什么对索引进行表达式计算，就无法走索引了呢？**

原因跟对索引使用函数差不多。

因为索引保存的是索引字段的原始值，而不是id+1表达式计算后的值，所以无法走所以，只能通过把索引字段的取值都取出来，然后依次进行表达式的计算来进行条件判断，因此采用的就是全表扫描的方式。

有的同学可能会说，这种对索引进行简单的表达式计算，在代码的特殊处理下，应该是可以做到索引扫描的，比方将id+1=10变成id=10-1。

是的，是能够实现，但是MySQL还是偷了这个懒，没有实现。

我的想法是，可能也是因为，表达式计算的情况多种多样，每种都要考虑的话，代码可能会很臃肿，所以干脆将这种索引失效的场景告诉程序员，让程序员自己保证在查询条件中不要对索引进行表达式计算。

#### 对索引隐式类型转换

如果索引字段是字符串类型，但是在条件查询中，输入的参数是整形的话，你会在执行计划的结果发现这个条语句会走全表扫描。

在原本的t_user表增加一个phone字段，是二级索引且类型是varhcar。

然后在查询条件中，用整形作为输入参数，此时执行计划中的type=ALL，所以是通过全表扫描来查询数据的。

```mysql
select * from t_user where phone = 1300000001;
```

但是如果索引字段是整形类型，查询条件中输入参数即使是字符串，是不会导致所有失效，还是可以走索引扫描。

我们再看第二个例子，id是整型，但是下面这条语句走的还是索引扫描的。

```mysql
explain select * from t_user where id = '1';
```

**为什么第一个例子会导致索引失效，而第二个例子不会呢？**

要明白这个原因，首先我们要直到MySQL的数据类型转换规则是什么？就是看MySQL是会将字符串转换成数字处理，还是将数字转换成字符串处理。

我在看《mysql45讲的时候》看到一个简单的测试方式，就是通过select"10">9的结果来知道MySQL的数据类型转换规则是什么：

- 如果规则是MySQL会自动将`字符串`转换成`数字`，就相当于select10>9，这个就是数字比较，所以结果应该是1；
- 如果规则是MySQL会自动将`数字`转换成`字符串`，就相当于select"10">"9"，这个是字符串比较，字符串比较大小是逐位从高位到低位逐个比较（按ascii码），因为“1”字符串比“9”字符串小，所以结果应该是0。

在MySQL中执行，select"10">9结果位1，说明**MySQL在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。**

前面的例子一中的查询语句，也跟大家说了是会走全表扫描：

```mysql
select * from t_user where phone = 1300000001;
```

这是因为phone字段为字符串，所以MySQL要会自动把字符串转为数字，所以这条语句相当于：

```mysql
select * from t_user where CAST(phone AS signed int) = 130000001;
```

可以看到，**CAST函数是作用在了phone字段，而phone字段是索引，也就是索引使用了函数！而前面我们也说了，对索引使用函数会导致索引失效的。**

例子二中的查询语句，跟大家说了是会走索引扫描：

```mysql
select * from t_user where id = "1";
```

这是因为字符串部分是输入参数，也就需要将字符串转为数字，所以这条语句相当于：

```mysql
select * from t_user where id = CAST("1" AS signed int);
```

可以看到，索引字段并没有用任何函数，CAST函数是用在了输入参数，因此是可以走索引扫描的。

#### 联合索引非最左匹配

对主键字段建立的索引叫聚簇索引，对普通字段建立的索引叫做二级索引。

那么**多个普通字段组合在一起创建的索引就叫做联合索引**，也叫组合索引。

创建联合索引时，我们需要注意创建时的顺序问题，因为联合索引（a，b，c）和（c，b，a）在使用的时候会存在差别。

联合索引要能正确使用需遵循**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配。

比如，如果创建了一个`(a, b, c)`联合索引，如果查询条件是以下几种，就可以匹配上联合索引：

- where a=1;
- where a=1 and b=2 and c=3;
- where a=1 and b=2;

需要注意的是，因为有查询优化器，所以a字段在where字句的顺序并不重要。

但是，如果查询条件是以下几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效：

- where b=2;
- where c=3;
- where b=2 and c=3;

有一个比较特殊的查询条件：where a=1 and c=3，符合最左匹配吗？

这种严格意义上来说属于索引截断，不同版本处理方式页不一样。

MySQL 5.5的话，前面a会走索引，在联合索引找到主键值后，开始回表，到主键索引读取数据行，Server层从存储引擎层获取到数据行后，然后在Server层再对比c字段的值。

从MySQL 5.6之后，有一个**索引下推功能**，可以在存储引擎层进行索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，再返回给Server层，从而减少回表次数。

索引下推的大概原理是：截断的字段不会在Server层进行条件判断，而是会被下推到`存储引擎层`进行条件判断（因此c字段的值是在`(a, b, c)`联合索引里的），然后过滤出符合条件的数据后再返回给Server层。由于在引擎层就过滤掉大量的数据，无序再回表读取数据来进行判断，减少回表次数，从而提高了性能。

比如下面这条where a=1 and c=0语句，我们可以从执行计划中的Extra=Using index condition使用了索引下推功能。

**为什么联合索引不遵循最左匹配原则就会失效？**

原因是，在联合索引的情况下，数据是按照索引第一列排序，第一列数据相同时才会按照第二列排序。

也就是说，如果我们想使用联合索引中尽可能多的列，查询条件中的各个列必须时联合索引中从最左边开始连续的列。如果我们仅仅按照第二列搜索，肯定无法走索引。

#### WHERE字句中的OR

在where字句中，如果在or前的条件列时索引列，而在or后的条件列不是索引列，那么所有会失效。

举个例子，比如下面的查询语句，id是主键，age是普通列，从执行计划的结果看，是走了全表扫描。

```mysql
select * from t_user where id = 1 or age = 18;
```

这是因为or的含义就是两个只要一个满足即可，因此只有一个条件列是索引列是没有意义的，只有有条件列不是索引列，就会进行全表扫描。

要解决方法很简单，将age字段设置为索引列即可。

可以看到type=index merge，index merge的意思就是对id和age分别进行了扫描，然后将这两个结果集进行了合并，这样做的好处就是为了避免全表扫描。

#### 总结

今天给大家介绍了6种会发生索引失效的情况：

- 当我们使用做或者左右模糊匹配的时候，也就是`like %xx`或者`like %xx%`这两种方式都会造成索引失效；
- 当我们在查询条件中对索引列使用函数，就会导致索引失效；
- 当我们在查询条件中对索引列进行表达式计算，也是无法走索引的；
- MySQL在遇到字符串和数字比较的时候，会自动将字符串转为数字，然后再进行比较。如果字符串是索引列，而条件语句中的输入参数是数字的话，那么索引列会发生隐式类型转换，由于隐式类型转换式通过CAST函数实现的，等同于对索引列使用了函数，所以就会导致索引失效；
- 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效；
- 在where字句中，如果在or前面的条件列式索引列，而在or后的条件列不是索引列，那么索引会失效。

### count(*)和count(1)有什么区别？哪个性能最好？

#### 哪种count性能最好？

直接说结论，按照性能排序：

count(*)=count(1)>count(主键字段)>count(字段)

要弄明白这个，我们得深入count的原理，以下内容基于常用的InnoDB存储引擎来说明。

##### count()是什么？

count()是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是**统计符合查询条件的记录中，函数指定的参数部位NULL的记录有多少个**。

假设count()函数的参数是字段名，如下：

```mysql
select count(name) from t_order;
```

这条语句是统计[t_order表中，name字段不为NULL的记录]有多少个。也就是说，如果某一条记录中的name字段的值为NULL，则不会被统计进去。

再来假设count()函数的参数是数字1这个表达式，如下：

```mysql
select count(1) form t_order;
```

这条语句是统计[t_order表中，1这个表达式部位NULL的记录]有多少个。

1这个表达式就是单纯数字，它永远都不是NULL。所以上面这条语句，其实是在统计t_order表中有多少个记录。

##### count(主键字段)执行过程是怎样的？

在通过count函数统计有多少个记录时，MySQL的Server层会维护一个名叫count的变量。

Server层会循环向InnoDB读取一条记录，如果count函数指定的参数不为NULL，那么就会将变量count加1，直到符合查询的全部记录被读完，就退出循环。最后将count变量的值发送给客户端。

InnoDB是通过B+树来保存记录的，根据索引的分类又分为聚簇索引和二级索引，它们区别在于，聚簇索引的叶子节点存放的是实际数据，而二级索引的叶子节点存放的是主键值，而不是实际数据。

用下面这条语句作为例子：

```mysql
select count(id) from t_order;
```

如果表里只有主键索引，没有二级索引，那么，InnoDB循环遍历聚簇索引，将读出到的记录返回给Server层，然后读取记录中的id值，判断id值是否为NULL，如果不为NULL，就将count变量加1。

但是，如果表里有二级索引时，InnoDB循环遍历的对象就不是聚簇索引，而是二级索引。

这时因为相同数量的二级索引记录可以比聚簇索引记录占用更少的存储空间，所以二级索引树比聚簇索引小，这样遍历二级索引的I/O成本比遍历聚簇索引的I/O成本小，因此`优化器`优先选择的是二级索引。

##### count(1)执行过程是怎样的？

用下面这条语句作为例子：

```mysql
select count(1) from t_order;
```

如果表里只有主键索引，没有二级索引时。

那么，InnoDB循环遍历聚簇索引（主键索引），将读取到的记录返回给Server层，**但是不会读取记录中的任何字段的值**，因为count函数的参数是1，不是字段，所以不需要读取记录中的字段值。参数1很明显并不是NULL，因此Server层每从InnoDB读取到一条记录，就将count变量加1。

可以看到，count(1)相比count(主键字段)少一个步骤，就是不需要读取记录中的字段值，所以通常会说count(1)执行效率会比count(主键字段)高一些。

但是，如果表里有二级索引时，InnoDB循环遍历的对象就是二级索引了。

##### count(*)执行过程是怎样的？

看到`*`这个字符的时候，是不是大家觉得是读取记录中的所有字段值？

对于`select * `这条语句来说是这个意思，但是在count(*)中并不是这个意思。

**count(`*`) 其实等于 count(`0`)**，也就是说，当你使用 count(`*`) 时，MySQL 会将 `*` 参数转化为参数 0 来处理。

所以，**count(\*) 执行过程跟 count(1) 执行过程基本一样的**，性能没有什么差异。

##### count(字段) 执行过程是怎样的？

count(字段) 的执行效率相比前面的 count(1)、 count(*)、 count(主键字段) 执行效率是最差的。

```mysql
// name不是索引，普通字段
select count(name) from t_order;
```

对于这个查询来说，会采用全表扫描的方式来计数，所以它的执行效率是比较差的。

##### 小结

count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。

所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。

再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。

### 事务篇

#### 事务的隔离级别是怎么实现的？

举一个转账过程的例子：

1. 从数据库读取我的余额
2. 将我的余额减去转账的金额
3. 将我修改后的余额更新到数据库里
4. 从余额读取你的余额
5. 将你的余额加上转账的金额
6. 将你修改的余额更新到数据库里

可以看到在转账的过程中会涉及到两次修改数据库的操作。

假设在执行第三步骤之后，服务器忽然掉电了，就会发生一个蛋疼的事情，我的账户扣了100万，但是钱并没有到你的账户，也就是说这**100万消失了！**

要解决这个问题，就要保证转账业务里的所有数据库的操作是不可分割的，要么全部执行成功，要么全部失败，不允许出现中间状态的数据。

数据库中的`事务（Transaction）`就能达到这样的效果。

我们在转账操作前先开启事务，等所有数据库操作执行完成后，才提交事务，对于已经提交的事务来说，该事务对数据库所作的修改将永久生效，如果中途发生终端或错误，那么该事务期间对数据库所做的修改将会被回滚到没执行该事务之前的状态。

#### 事务有哪些特性？

事务是由MySQL的存储引擎来实现的，我们常见的InnoDB引擎它是支持事务的。

不过并不是说有引擎都能支持事务，比如MySQL原生的MyISAM引擎就不支持事务，也正是这样，所以大多数MySQL的引擎都是InnoDB。

事务看起来感觉简单，但是要实现事务必须遵守4个特性，分别如下：

- 原子性（Atomicity）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。
- 一致性（Consistency）：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户A和用户B在银行分别由800元和600元，总共1400元，用户A给用户B转账200元，分为两个步骤，从A的账户扣除200元和对B的账户增加200元。一致性就是要求上述步骤操作后，最后的结果时用户A还有600元，用户B有800元，总共1400元，而不会出现用户A扣除了200元，但用户B未增加的情况（该情况，用户A和B均为600元，总共1200元）。
- 隔离性（lsolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。
- 持久性（Durability）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

这次将**重点介绍事务的隔离性**，这也是面试时最常问的知识点。

为什么事务要有隔离性，我们就要直到并发事务时会引发什么问题。

#### 并行事务会引发什么问题？

MySQL服务端时允许多个客户端连接的，这意味着MySQL会出现同时处理多个事务的情况。

那么**在同时处理多个事务的时候，就可能出现脏读（dirty read）、不可重复度（no-repeatable read）、幻读（phantom read）的问题。**

接下来，通过举例子给大家说明，这些问题是如何发生的。

##### 脏读

**如果一个事务`读到`了另一个`未提交事务修改过的数据`，就意味着发生了`脏读`现象。**

举个例子，假设有A和B这两个事务同时在处理，事务A先从数据库中读取小林的余额数据，然后再执行更新操作，如果此时事务A还没提交事务，而此时事务B页从数据库中读取小林的余额数据，那么事务B读取到的余额数据是刚才事务A更新后的数据，即使没有提交事务。

因为事务A是还没提交事务的，也就是它随时可能发生回滚操作，**如果在上面这种情况事务A发生了回滚，那么事务B刚才得到的数据就是过期数据，这种现象就被称为脏读。**

##### 不可重复度

**在一个事务内多次读取同一个数据，如果出现前后两次读取到的数据不一样的情况，就意味着发生了`不可重复读`现象。**

举个例子，假设有A和B这两个事务同时在处理，事务A先从数据库中读取小林的余额数据，然后继续执行代码逻辑处理，**在这个过程中如果事务B更新了这条数据，并提交了事务，那么当事务A再次读取到该数据时，就会发现前后两次读取到的数据是不一致的，这种现象就被称为不可重复度。**

##### 幻读

**在一个事务内多次查询某个符合条件的`记录数量`，如果出现前后两次查询到的记录数量不一样的情况，就以为着发生了`幻读`现象。**

举个例子，假设有A和B这两个事务同时在处理，事务A先开始从数据库查询账户余额大于100万的记录，发现共有5条，然后事务B页按相同的搜索条件也是查询出了5条记录。

接下来，事务A插入了一条余额超过100W的账号，并提交了事务。此时数据库超过100万余额的账号个数就变为6。

然后事务B再次查询账户余额大于100万的记录，此时查询到的记录数量有6条，**发现和前一次读到的记录数量不一样了，就感觉发生了幻觉一样，这种现象就被称为幻读。**

#### 事务的隔离级别有哪些？

前面我们提到，当戈多事务并发执行时可能会遇到[脏读、不可重复读、幻读]现象，这些现象会对事务的一致性产生不同程度的影响。

- 脏读：读到其他事务未提交的数据；
- 不可重复读：前后读取的数据不一致；
- 幻读：前后读取数据的记录数量不一致；

这三个现象的严重性排序如下：脏读>不可重复度>幻读

SQL标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：

- 读未提交（read uncommited），指一个事务还没提交时，它做的变更就能被其他事务看到；
- 读提交（read commited），指一个事务提交之后，它做的变更才能被其他事务看到；
- 可重复读（repeatable read），指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB引擎的默认隔离级别；
- 串行化（serializable），会对记录加上读写锁，在多个事务多这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

按隔离水平高低排序如下：串行化>可重复读>读已提交>读未提交

针对不同的隔离级别，并发事务时可能发生的现象也会不同。

也就是说：

- 在[读未提交]隔离级别下，可能发生脏读、不可重复度和幻读现象。
- 在[读提交]隔离级别下，可能发生不可重复读和幻读现象，但是不可能发生脏读现象；
- 在[可重复读]隔离级别下，可能发生幻读现象，但是不可能发生脏读和不可重复读现象；
- 在[串行化]隔离级别下，脏读、不可重复读和幻读现象都不可能会发生。

所以，要解决脏读现象，就要升级到`读提交`以上的隔离级别；要解决不可重复读现象，就要升级到`可重复读`的隔离级别，**我们讨论的MySQL虽然支持4种隔离级别，但是于SQL标准中规定的各级隔离级别允许发生的现象却有些出入。**

MySQL在[可重复读]隔离级别下，可以很大程度避免幻读现场的发生（注意是很大程度避免，并不是彻底避免），所以MySQL并不会使用[串行化]隔离级别来避免幻读现象的发生，因为使用[串行化]隔离级别会影响性能。

**MySQL InnoDB引擎的默认隔离级别虽然是[可重复读]，但是它很大程度上避免幻读现象（并不是完全解决了）**，解决的方案有两种：

- 针对**快照读**（普通select语句），是**通过MVCC方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查不出这条数据的，所以就很好得避免了幻读问题。
- 针对**当前读**（select ... from update等语句），是**通过next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行select ... for update语句的时候，会加上next-key lock，如果有其他事务在next-key lock锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好避免了幻读问题。

接下来，举个具体的例子来说明这四种隔离级别，有一张账户余额表，里面有一条账户余额为100万的记录。然后有两个并发的事务，事务A只负责查询余额，事务B则会将我的余额改为200万，

在不同隔离级别下，事务A执行过程中查询到的余额可能会不同：

- 在[读未提交]隔离级别下，事务B修改余额后，虽然没有提交事务，但是此时的余额已经可以被事务A看见了，于是事务 A 中余额 V1 查询的值是 200 万，余额 V2、V3 自然也是 200 万了；
- 在[读提交]隔离级别下，事务B修改余额后，因为没有提交事务，所以事务A中余额V1的值还是100万，等事务B提交完后，最新的余额数据才能被事务A看到，因此余额V2、V3都是200万；
- 在[可重复读]隔离级别下，事务A只能看见启动事务时的数据，所以余额V1、余额V2的值都是100万，当事务A提交事务后，就能看到最新的余额数据了，所以余额V3的值是200万；
- 在[串行化]隔离级别下，事务B在执行将余额100万修改为200万时，由于此前事务A执行了读操作，这样就发生了读写冲突，于是就会被锁住，直到事务A提交后，事务B才可以继续执行，所以从A的角度看，余额V1、V2的值时100万，余额V3的值是200万。

这四种隔离级别具体是如何实现的呢？

- 对于[读未提交]隔离级别的事务来说，因为可以读到未提交事务修改的数据，所以直接读取最新的数据就好了；
- 对于[串行化]隔离级别的事务来说，通过加锁写锁的方式来避免并行访问；
- 对于[读提交]和[可重复读]隔离级别的事务来说，它们是通过**Read View来实现的，它们的区别在于创建Read View的时机不同，大家可以把Read View理解成一个数据快照，就像照相机拍照那样，定格某一时刻的风景。[读提交]隔离级别是在[每个语句执行前]都会重新生成一个Read View，而[可重复读]隔离级别是[启动事务时]生成一个Read View，然后整个事务期间都在用这个Read View。**

注意，执行[开始事务]命令，并不意味着启动了事务。在MySQL有两种开启事务的命令，分别是：

- 第一种：begin/start transaction命令；
- 第二种：start transaction with consistent snapshot命令；

这两种开启事务的命令，事务的启动时机是不同的：

- 执行了begin/start transaction命令后，并不代表事务启动了。只有执行了这个命令后，执行了增删查改操作的SQL语句，才是事务真正启动的时机；
- 执行了start transaction with consistent snapshot命令，就会马上启动事务。

接下来详细说下，Read View在MVCC里如何工作的？

#### Read View在MVCC里是如何工作的？

我们需要了解两个知识：

- Read View 中四个字段的作用；
- 聚簇索引记录中两个跟事务有关的隐藏列；

那Read View到底是个什么东西？

Read View 有四个重要的字段：

- m_ids：指的是在创建Read View时，当前数据库中[活跃事务]的**事务id列表**，注意是一个列表，**“活跃事务”指的就是，启动了但还没提交的事务。**
- min_trx_id：指的是在创建Read View时，当前数据库中[活跃事务]中事务**id最小的事务**，也就是m_ids的最小值。
- max_tri_id：这个并不是m_ids的最大值，而是**创建Read View时当前数据库中应该给下一个事务的id值**，也就是全局事务中最大的事务id值+11；
- creator_tri_id：指的是**创建该Read View事务的事务id**。

知道了Read View的字段，我们还需要了解聚簇索引记录中的两个隐藏列。

对于使用InnoDB存储引擎的数据库表，它的聚簇索引记录中都包含下面两个隐藏列：

- trx_id，当一个事务对某条聚簇索引记录进行改动时，就会**把该事务的事务id记录在trx_id隐藏列里**；
- roll_pointer：每当对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到undo日志中，然后**这个隐藏列是个指针，指向每一个旧版本的记录**，于是就可以通过它找到修改前的记录。

在创建Read View后，我们可以将记录中的trx_id划分这三种情况：

一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：

- 如果记录的trx_id值小于Read View中的`min_trx_id`值，表示这个版本的记录是创建在Read View**前**已经提交的事务生成的，所以该版本的记录对当前事务**可见**。
- 如果记录的trx_id值大于等于Read View中的`max_trx_id`值，表示这个版本的记录是在创建Read View**后**启动的事务生成的，所以该版本的记录对当前事务**不可见**。
- 如果记录的trx_id值在Read View的`min_trx_id`和`max_trx_id`之间，需要判断trx_id是否在m_ids列表中：
  - 如果记录的trx_id在`m_ids`列表中，表示生成该版本记录的活跃事务依然还活跃着（还没提交事务），所以该版本的记录对当前事务**不可见**。
  - 如果记录的trx_id不再`m_ids`列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务**可见**。

**这种通过[版本链]来控制并发事务访问同一个记录时的行为就叫MVCC（多版本并发控制）。**

#### 可重复读是如何工作的？

**可重复读隔离级别时启动事务时生成一个Read View，然后整个事务期间都在用这个Read View**。

假设事务A（事务id为51）启动后，紧接着事务B（事务id为52）也启动了，事务A和事务B的Read View具体内容如下：

- 在事务A的Read View中，它的事务id是51，由于它是第一个启动的事务，所以此时活跃事务的事务id列表就只有51，活跃事务的事务id列表最小的事务id是事务A本身，下一个事务id则是52。
- 在事务B的Read View中，它的事务id是52，由于事务A是活跃的，所以此时活跃事务的事务id列表是51和52，**活跃的事务id中最小的事务id是事务A**，下一个事务id应该是53。

接着，在可重复读隔离级别下，事务A和事务B按顺序执行了一下操作：

- 事务B读取小林的账户余额记录，读到余额是100万；
- 事务A将小林的账户余额记录修改成200万，并没有提交事务；
- 事务B读取到小林的账户余额记录，读到余额还是100万；
- 事务A提交事务；
- 事务B读取小林的账号余额记录，读到余额依然还是100万；

接下来，跟大家具体分析一下。

事务B第一次读到小林的账号余额记录，在找到记录后，它会先看这条记录的trx_id，此时**发现trx_id为50，比事务B的Read View中的min_trx_id值（51）还小，这意味着修改这条记录的事务早就在事务B启动前就提交过了，所以该版本的记录对事务B可见的，**也就是事务B可以获取到这条记录。

接着，事务A通过update语句将这条记录修改了（还未提交事务），将小林的余额改成200万，这时MySQL会记录相应的undo log，以链表的方式串联起来，形成**版本链**。

由于事务A修改了该记录，以前的记录就变成旧版本的记录了，于是新记录和旧记录通过链表的方式串联起来，而且最新记录的trx_id是事务A的id（trx_id=51）。

然后事务B第二次去读取该记录，**发现这条记录的trx_id值为51，在事务B的Read View的min_trx_id和max_trx_id之间，则需要判断trx_id值是否在m_ids范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务B并不会读取这个版本的记录，而是沿着undo log链表往下找旧版本的记录，知道找到trx_id[小于]事务B的Read View中的min_tri_id值的第一条记录，**所以事务B能读取到的是trx_id为50的记录，也就是小林余额是100万这条记录。

最后，当事务A提交事务后，**由于隔离级别是[可重复读]，所以事务B再次读取记录时，还是基于启动事务时创建的Read View来判断当前版本的记录是否可见。所以，即使事务A将小林余额修改为200万并提交了事务，事务B第三次读取记录时，读到的记录都是小林余额时100万的这条记录。**

就是通过这样的方式实现了，[可重复读]隔离级别下的事务期间读到的记录都是事务启动前的记录。

#### 读提交是如何工作的？

**读提交隔离级别时每次读数据时，都会生成一个新的Read View。**

也意味着，事务期间的多次读取同一条数据，前后两次读出来的数据可能会出现不一致，因为可能这七家你另外一个事务修改了该记录，并提交了事务。

那读提交隔离级别是怎么工作的呢？我们还是以前面的例子来聊聊。

假设事务A（事务id为51）启动后，紧接着事务B（事务id为52）也启动了，接着按顺序执行了以下操作：

- 事务B读取数据（创建Read View），小林的账户余额为100万；
- 事务A修改数据（还没提交事务），将小林的账户余额从100万修改成了200万；
- 事务B读取数据（创建Read View），小林的账户余额为100万；
- 事务A提交事务；
- 事务B读取数据（创建Read View），小林的账户余额为200万；

我们来分析一下为什么事务B第二次读数据时，读不到事务A（还未提交事务）修改的数据？

事务B在找到小林这条记录时，会看这条记录的trx_id是51，在事务B的Read View的min_tri_id和max_trx_id之间，接下来判断trx_id值是否在m_ids范围内，判断结果是在的，那么说明**这条记录时被还未提交的事务修改的，这时事务B并不会读取这个版本的记录。**而是，沿着undo log链表往下找旧版本的记录，直到找到trx_id[小于]事务B的Read View中的min_trx_id值的第一条记录，所以事务B能读取到的是trx_id为50的记录，也就是小林余额是100万这条记录。

我们来分析一下为什么事务A提交后，事务B就可以读到事务A修改的数据？

在事务A提交后，**由于隔离级别是[读提交]，所以事务B在每次读取数据的时候，会重新创建Read View，**此时事务B第三次读取数据时创建的Read View如下：

事务B在找到小林这条记录时，**会发现这条记录的trx_id是51，比事务B的Read_View中的mix_trx_id值（52）还小，这意味着修改这条记录的事务早就在创建Read View前提交过了，所以该版本的记录对事务B是可见的。**

正是因为在读提交隔离界别下，事务每次读数据时都会重新创建Read View，那么在事务期间多次读取同一条数据，前后两个读到的数据可能会出现不一致，因为可能这个期间另一个事务修改了该记录，并提交了事务。

#### 总结

事务是在MySQL引擎层实现的，我们常见的InnoDB引擎是支持事务的，事务的四大特性是原子性、一致性、隔离性、持久性，我们这次主要讲的是隔离性。

当多个事务并发执行的时候，会引发脏读、不可重复读、幻读这些问题，那为了避免这些问题，SQL提出了四种隔离级别，分别是读未提交、读已提交、可重复读、串行化，从左往右隔离级别顺序递增，隔离级别越高，意味着性能越差，InnoDB引擎默认的隔离级别的是可重复读。

要解决脏读现象，就要讲隔离级别提交到读已提交以上隔离级别，要解决不可重复读现场，就要讲隔离界别升级到可重复读以上的隔离级别。

而对于幻读现象，不建议讲隔离级别升级为串行化，因为这回导致数据并发时性能很差。MySQL InnoDB引擎的默认隔离级别虽然是[可重复读]，但是它很大程度避免了幻读现象（并不是完全解决了），解决的方案有两种：

- 针对**快照读**（普通select语句），是通过**MVCC方式解决了幻读，**因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好避免了幻读问题。
- 针对**当前读**（select ... for update等语句），是**通过next-key lock（记录锁+间隙锁）方式解决了幻读，**因为当执行select ... for update语句的时候，会加上next-key lock，如果有其他事务在next-key lock锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好避免了幻读问题。

对于[读提交]和[可重复读]隔离级别事务来说，它们是通过Read View来实现的，它们的区别在于创建Read View的时机不同：

- [读提交]隔离级别是在每个select都会生成一个新的Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这个期间另一个事务修改了该记录，并提交了事务。
- [可重复读]隔离级别是启动事务时生成一个Read View，然后整个事务期间都在用这个Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。

这两个隔离级别实现是通过[事务的Read View里的字段]和[记录中的两个隐藏列]的比对，来控制并发事务访问同一个记录的i行为，这就叫MVCC（多版本并发控制）。

在可重复读隔离级别中，普通的select语句就是基于MVCC实现的快照读，也就是不会加锁的。而select ... for update语句就不是快照读了，而是当前读，也就是每次读都是拿到最新版本的数据，但是它会对读到的记录加上next_key lock锁。

### MySQL可重复读隔离级别，完全解决了幻读吗？

在上文提到，MySQL InnoDB引擎的默认隔离级别虽然是[可重复读]，但是它很大程度上避免了了幻读现场（并不是完全解决了），解决的方案有两种：

- 针对**快照读**（普通select语句），是通过**MVCC方式解决了幻读，**因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入一条数据，是查不出这条数据的，所以就很好避免了幻读问题。
- 针对**当前读**（select ... for update等语句），就是**通过next-key lock（记录锁+间隙锁）方式解决了幻读，**因为当执行select ... for update 语句的时候，会加上next-key lock，如果有其他事务在next-key lock锁范围内插入一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好避免了幻读问题。

这两个解决方案是很大程度解决了幻读现象，但是还是有个别情况造成的幻读现象是无法解决的。

#### 什么是幻读？

首先来看看MySQL文档是怎么定义幻读的：当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。

举个例子，假设一个事务在 T1 时刻和 T2 时刻分别执行了下面查询语句，途中没有执行其他任何语句：

```sql
SELECT * FROM t_test WHERE id > 100;
```

只要 T1 和 T2 时刻执行产生的结果集是不相同的，那就发生了幻读的问题，比如：

- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 6 条行记录，那就发生了幻读的问题。
- T1 时间执行的结果是有 5 条行记录，而 T2 时间执行的结果是有 4 条行记录，也是发生了幻读的问题。

#### 快照读是如何避免幻读的？

可重复读隔离级别是由MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行begin语句后），在执行第一个查询语句后，会创建一个Read View，**后续的查询语句利用这个Read View，通过这个Read View就可以在undo log版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，**即使中途有其他事务插入了新记录，是查不出这条数据的，所以就很好避免了幻读问题。

#### 当前读是如何避免幻读的？

MySQL里除了普通查询时快照读，其他都是**当前读**，比如update、insert、delete，这些语句在执行前都会查询最新版本的数据库，然后再做进一步操作。

这很好理解，假设你要update一个记录，另一个事务已经delete这条记录并提交事务了，这样不是会产生冲突吗，所以update的时候肯定要知道最新的数据。

另外，`select ... for update`这种查询语句是当前读，每次执行的时候都是读取最新的数据。

接下来，我们假设`select .. for update`当前读是不会加锁的（实际上是会加锁的），在做一遍实验。

事务B插入的记录，会被事务A的第二条查询语句查询到（因为是当前读），这样就会出现前后两次查询结果的集合不一样，这样就出现了幻读。

所以，**InnoDB引擎为了解决[可重复读]隔离级别使用[当前读]而造成的幻读问题，就引出了间隙锁。**

假设，表中有一个范围id为（3，5）间隙锁，那么其他事务就无法插入id=4这条记录了，这样就有效防止幻读现象的发生。

事务A执行了`select name from t_stu where id > 2 for update`后，就在对表中的记录加上id范围为（2，正无穷大]的next-key lock（next-key lock是间隙锁+记录锁的组合）。

然后，事务B在执行插入语句的时候，判断到插入的位置被事务A加上了next-key lock，于是事务B会生成一个插入意向锁，同时进入等待状态，知道事务A提交了事务。这就避免了由于事务B插入新纪录而导致事务A发生幻读的现象。

#### 幻读被完全解决了吗？

**可重复读隔离级别下虽然很大程度避免了幻读，但是还是没能完全解决幻读。**

##### 第一个发生幻读现象的场景

事务A先查询id=5的记录，此时表中是没有该记录的，所以查询不出来。然后事务B插入一条id=5的记录，并且提交了事务。此时，**事务A更新id=5这条记录，对没错，事务A看不到id=5这条记录，但是他去更新了这条记录，着场景确实很违和，然后再次查询id=5的记录，事务A就能看到事务B插入的记录了，幻读就是发生在这种违和的场景。**

在可重复读隔离级别下，事务A第一次执行普通的select语句时生成了一个Read View，之后事务B向表中新插入了一条id=5的记录并提交。接着，事务A对id=5这条记录进行了更新操作，在这个时刻，这条新纪录的trx_id隐藏列的值就变成了事务A的事务id，之后事务A再使用普通select语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

因为这种特殊现象的存在，所以我们认为MySQL InnoDB中的MVCC并不能完全避免幻读现象。

##### 第二个发生幻读现象的场景

除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。

- T1时刻：事务A先执行[快照读语句]，select * from t_test where id > 100 得到了3条记录。
- T2时刻：事务B往表中插入一个id=200的记录并提交；
- T3时刻：事务A再执行[当前读语句] ，select * from t_test where id > 100 for update 就会得到4条记录，此时也发生了幻读现象。

**要避免这类特殊场景发生幻读现象的话，就是尽量再开启事务之后，马上执行select ... for update这类当前读语句，**因为它会对记录加next-key lock，从而避免其他事务插入一条新纪录。

#### 总结

MySQL InnoDB引擎的可重复读隔离级别（默认隔离级别），根据不同的查询方式，分别提出了避免幻读的方案：

- 针对**快照读**（普通select语句），是通过MVCC方式解决了幻读。
- 针对**当前读**（select ... for update等语句），是通过next-key lock（记录锁加间隙锁）方式解决了幻读。

我举例了两个发生幻读场景的例子。

第一个例子：对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。

第二个例子：对于当前读，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。

所以，**MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。**

要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

## 锁篇

### MySQL有哪些锁？

在MySQL里，根据加锁的范围，可以分为**全局锁、表级锁和行锁**三类。

#### 全局锁

**全局锁是怎么用的？**

要使用全局锁，则要执行这条命令：

```mysql
flush tables with read lock
```

执行后，**整个数据库就处于只读状态了，**这时其他线程执行以下操作，都会被阻塞：

- 对数据的增删改操作，比如insert、delete、update等语句；
- 对表结构的更改操作，比如alter table、drop table等语句。

如果要释放全局锁，则要执行这条命令：

```mysql
unlock tables
```

当然，会话断开了，全局锁会被自动释放。

**全局锁应用场景是什么？**

全局锁主要应用于做**全库逻辑备份**，这样在备份数据库期间，不会因为数据或表结构更新，而出现备份文件的数据于预期的不一样。

**加上全局锁又会带来什么缺点呢？**

加上全局锁，意味着整个数据库都是只读状态。

那么如果数据库里有很多数据，备份就会花费很多时间，关键是备份期间，业务只能数据，而不能更新数据，这样会造成业务停滞。

**既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？**

有的，如果数据库的引擎支持的事务支持**可重复读的隔离级别，**那么在备份数据库之前先开启事务，会先创建Read View，然后整个事务执行期间都在用这个Read View，而且由于MVCC的支持，备份期间业务依然可以对数据进行更新操作。

因为在可重复读的隔离级别下，即使其他事务更新了表的数据，也不会影响备份数据库时的Read View，这就是事务四大特性中的隔离性，这样备份期间备份的数据就一直是开启事务时的数据。

备份数据库的工具时mysqldump，在使用mysqldump时加上`-single-transaction`参数的时候，就会在备份数据库之前开启事务。这种方法只适用于支持[可重复读隔离级别的事务]的存储引擎。

InnoDB存储引擎默认的事务隔离级别正是可重复读，因此可以采用这种方式来备份数据库。

但是，对于MyISAM这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。

#### 表级锁

**MySQL表级锁有哪些？具体怎么用的。**

MySQL里面表级别的锁有这几种：

- 表锁；
- 元数据锁（MDL）；
- 意向锁；
- AUTO-INC锁；

##### 表锁

先来说说表锁。

如果我们想对学生表（t_student）加上表锁，可以使用下面的命令：

```mysql
// 表级别的共享锁，也就是读锁；
lock tables t_student read;

// 表级别的独占锁，也就是写锁；
lock tables t_student write;
```

需要注意的是，表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。

也就是说如果本线程对学生加了[共享表锁]，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程表学生表进行写操作时也会被阻塞，知道锁被释放。

要释放表锁，可以使用下面这条命令，会释放当前会话的所有表锁：

```mysql
unlock tables
```

另外，当会话退出后，也会释放所有表锁。

不过尽量避免在使用InnoDB引擎的表使用表锁，因为表锁的颗粒度太大，会影响并发性能，**InnoDB牛逼的地方在于实现了颗粒度更细的行级锁。**

##### 元数据锁

再来说说**元数据锁（MDL）。**

我们不需要显示的使用MDL，因为当我们对数据库表进行操作时，会自动给这个表加上MDL：

- 对一张表进行CRUD操作时，加的锁是**MDL读锁**；
- 对一张表做结构变更操作的时候，加的锁是**MDL写锁**；

MDL是为了保证当用户对表执行CRUD操作时，防止其他线程对这个表结构做了变更。

当有线程在执行select语句（加MDL读锁）的期间，如果有其他线程要更改该表的结构（申请MDL写锁），那么将会被阻塞，直到执行完select语句（释放MDL读锁）。

反之，当有线程对表结构进行变更（加MDL写锁）的期间，如果有其他线程执行了CRUD操作（申请MDL读锁），那么就会被阻塞，直到表结构变更完成（释放MDL写锁）。

**MDL不需要显示调用，那它是在什么时候释放的？**

MDL是在事务提交后才释放，这意味着**事务执行期间，MDL是一只持有的。**

那如果数据库有一个长事务（所谓的长事务，就是开启了事务，但是一直还没提交），那在对表结构做变更操作的时候，可能会发生意想不到的事情，比如下面这个循序的场景：

1. 首先，线程 A 先启用了事务（但是一直不提交），然后执行一条 select 语句，此时就先对该表加上 MDL 读锁；
2. 然后，线程 B 也执行了同样的 select 语句，此时并不会阻塞，因为「读读」并不冲突；
3. 接着，线程 C 修改了表字段，此时由于线程 A 的事务并没有提交，也就是 MDL 读锁还在占用着，这时线程 C 就无法申请到 MDL 写锁，就会被阻塞，

那么线程C阻塞后，后续有对该表的select语句，就都会被阻塞，如果此时有大量该表的select语句的请求到来，就会有大量的线程被阻塞，这时数据库的线程很快就会爆满了。

**为什么线程C因为申请不到MDL写锁，而导致后续的申请读锁的查询操作也会被阻塞？**

这是以你为申请MDL锁的操作会形成一个队列，队列中**写锁获取优先级高于读锁，**一旦出现MDL写锁等待，会阻塞后续该表的所有CRUD操作。

所以为了能安全的对表结构进行变更，在对表结构变更前，先要看看数据库中的长事务，是否有事务已经对表加上了MDL读锁，如果有可以考虑kill掉这个长事务，然后再对表结构做变更。

##### 意向锁

接着，说说**意向锁。**

- 在使用InnoDB引擎的表里做某些记录加上[共享锁]之前，需要先在表级别上加一个[意向共享锁]；
- 在使用InnoDB引擎的表里对某些记录加上[独占锁]之前，需要先在表级别上加上一个[意向独占锁]；

也就是，当执行插入、更新、删除操作，需要先对表加上[意向独占锁]，然后对该记录加上独占锁。

而普通的select是不会加行级锁的，普通的select语句是利用MVCC实现一致性读，是无锁的。

不过，select也是可以对记录加共享锁和独占锁的，具体方式如下：

```mysql
//先在表上加上意向共享锁，然后对读取的记录加共享锁
select ... lock in share mode;

//先表上加上意向独占锁，然后对读取的记录加独占锁
select ... for update;
```

未完....

## 日志篇

### MySQL日志：undo log、redo log、binlog有什么用？

**执行一条update语句，期间发生了什么？**比如下面这条update语句：

````mysql
UPDATE t_user SET name = 'xiaolin' WHERE id = 1;
````

查询语句的那套流程，更新语句也是同样会走一遍：

- 客户端先通过连接器建立连接，连接器会判断用户身份；
- 因为这是一条update语句，所以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存清空，所以查询缓存很鸡肋，在MySQL8.0就被移除这个功能了；
- 解析器会通过词法分析识别出关键字update，表名等等，构建出语法树，接着还会做语法分析，判断输入的语句是否符合MySQL语法；
- 预处理器会判断表和字段是否存在；
- 优化器确定执行计划，因为where条件中的id是主键索引，所以决定要使用id这个索引；
- 执行器负责具体执行，找到这一行，然后更新。

不过，更新语句的流程会涉及到undo log（回滚日志）、redo log（重做日志）、binlog（归档日志）这三种日志：

- **undo log（回滚日志）：**是InnoDB存储引擎层生成的日志，实现了事务中的**原子性，**主要**用于事务回滚和MVCC；**
- **redo log（重做日志）：**是InnoDB存储引擎层生成的日志，实现了事务中的**持久性，**主要**用于掉电等故障恢复；**
- **binlog（归档日志）：**是Server层生成的日志，主要**用于数据备份和主从复制；**

所以这次就带着这个问题，看看这三种日志是怎么工作的。

#### 为什么需要undo log？

我们在执行一条“增删改”语句的时候，虽然没有输入begin开启事务和commit提交事务，但是MySQL会**隐式开启事务**来执行“增删改”语句的，执行完后就自动提交事务，这样就保证了执行完“增删改”语句后，我们可以及时在数据库表中看到“增删改”的结果了。

执行一条语句是否自动提交事务，是由`autocommit`参数决定的，默认是开启的。所以，执行一条update语句也是会使用事务的。

那么，考虑一个问题，一个事务在执行过程中，在还没有提交事务之前，如果MySQL发生了崩溃，要怎么回滚到事务之前的数据呢？

如果我们每次在事务的执行过程中，都记录回滚时需要的信息到一个日志里，那么在事务执行中途发生了MySQL崩溃后，就不用担心无法回滚到事务之前的数据，我们可以通过这个日志回滚到事务之前的数据。

实现这一机制就是**undo log（回滚日志），它保证了事务的ACID特性中的原子性（Atomicity）。**

undo log是一种用于撤销回退的日志。在事务没提交之前，MySQL会先记录更新前的数据到undo log日志文件里，当事务回滚时，可以利用undo log来进行回滚。

每当InnoDB引擎对一条记录进行操作（删除、修改、新增）时，要把回滚时需要的信息都记录到undo log里，比如：

- 在**插入**一条记录时，要把这条记录的主键值记录下来，这样之后回滚时只需要把这个主键值对应的记录**删掉**就好了；
- 在**删除**一条记录时，要把这条记录的内容都记下来，这样之后回滚时再把由这些内容组成的记录**插入**到表中就好了；
- 在**更新**一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列**更新为旧值**就好了。

在发生回滚时，就读取undo log里的数据，然后做原先相反操作。比如当 delete 一条记录时，undo log 中会把记录中的内容都记下来，然后执行回滚操作的时候，就读取 undo log 里的数据，然后进行 insert 操作。

不同的操作，需要记录的内容也是不同的，所以不同类型的操作（修改、删除、新增）产生的 undo log 的格式也是不同的，具体的每一个操作的 undo log 的格式我就不详细介绍了，感兴趣的可以自己去查查。

一条记录的每一次更新操作产生的undo log格式都有一个roll_pointer指针和一个trx_id事务id：

- 通过trx_id可以知道该记录时被哪个事务修改的；
- 通过roll_pointer指针可以将这些undo log串成一个链表，这个链表就被称为版本链；

另外，**undo log还有一个作用，通过Read View + undo log 实现MVCC（多版本并发控制）。**

对于[读提交]和[可重复读]隔离级别的事务来说，它们的快照读（普通select语句）是通过Read View + undo log来实现的，它们的区别在于创建Read View的时机不同：

- [读提交]隔离级别是在每个select都会生成一个新的Read View，也意味着，事务期间的多次读取同一条数据，前后两次读到的数据可能会出现不一致，因为可能这期间另一个事务修改了该记录，并提交了事务。
- [可重复读]隔离级别是启动事务时生成一个Read View，然后整个事务期间都在用这个Read View，这样就保证了在事务期间读到的数据都是事务启动前的数据。

这两个隔离级别实现是通过[事务的Read View里的字段]和[记录中的两个隐藏列（trx_id）和roll_pointer]的对比，如果不满足可见性，就会顺着undo log版本链里找到满足其可见性的记录，从而控制并发事务访问同一个记录时的行为，这就叫MVCC（多版本并发控制）。

因此，undo log两大作用：

- **实现事务回滚，保障事务的原子性**。事务处理过程中，如果出现了错误或者用户执行了ROLLBACK语句，MySQL可以利用undo log中的历史数据将数据恢复到事务开始之前的状态。
- **实现MVCC（多版本并发控制）关键因素之一。**MVCC是通过ReadView+undo log实现的。undo log为每条记录保存多份历史数据，MySQL在执行快照读（普通select语句）的时候，会根据事务的Read View里的信息，顺着undo log的版本链找到满足其可见性的记录。

**TIP**

**很多人疑问 undo log 是如何刷盘（持久化到磁盘）的？**

**undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。**

**buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。**

#### 为什么需要Buffer Pool？

MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。那修改完这条记录是选择直接写回到磁盘，还是选择缓存起来呢？

当然是缓存起来好，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。

为此，Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

有了 Buffer Pool 后：

- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
- 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。

#### Buffer Pool缓存什么？

InnoDB 会把存储的数据划分为若干个「页」，以页作为磁盘和内存交互的基本单位，一个页的默认大小为 16KB。因此，Buffer Pool 同样需要按「页」来划分。

在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。

所以，MySQL 刚启动的时候，你会观察到使用的虚拟内存空间很大，而使用到的物理内存空间却很小，这是因为只有这些虚拟内存被访问后，操作系统才会触发缺页中断，申请物理内存，接着将虚拟地址和物理地址建立映射关系。

Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息等等。

**Undo 页是记录什么？**

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

**查询一条记录，就只需要缓冲一条记录吗？**

不是的。

当我们查询一条记录时，InnoDB 是会把整个页的数据加载到 Buffer Pool 中，将页加载到 Buffer Pool 后，再通过页里的「页目录」去定位到某条具体的记录。

### 为什么需要 redo log ？

Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。

为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，**这个时候更新就算完成了**。

后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 **WAL （Write-Ahead Logging）技术**。

**WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上**。

**什么是 redo log？**

redo log 是物理日志，记录了某个数据页做了什么修改，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志。

在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。

当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。

**被修改 undo 页面，需要记录对应 redo log 吗？**

需要的。

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

不过，**在内存修改该 Undo 页面后，需要记录对应的 redo log**。

**redo log 和 undo log 区别在哪？**

这两种日志是属于InnoDB存储引擎的日志，它们的区别在于：

- redo log 记录了此次事务[**完成后**]的数据状态，记录的是更新**之后**的值；
- undo log 记录了此次事务[**开始前**]的数据状态，记录的是更新**之前**的值；

事务提交之前发生了崩溃，重启后会通过undo log回滚事务，事务提交后发生了崩溃，重启后会通过redo log恢复事务。

所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 **crash-safe**（崩溃恢复）。可以看出来， **redo log 保证了事务四大特性中的持久性**。

**redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？**

写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是**随机写**。

磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。

针对「顺序写」为什么比「随机写」更快这个问题，可以比喻为你有一个本子，按照顺序一页一页写肯定比写一个字都要找到对应页写快得多。

可以说这是 WAL 技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。

至此， 针对为什么需要 redo log 这个问题我们有两个答案：

- **实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；
- **将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。

**产生的 redo log 是直接写入磁盘的吗？**

不是的。

实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。

所以，redo log 也有自己的缓存—— **redo log buffer**，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘。

redo log buffer 默认大小 16 MB，可以通过 `innodb_log_Buffer_size` 参数动态的调整大小，增大它的大小可以让 MySQL 处理「大事务」是不必写入磁盘，进而提升写 IO 性能。

#### redo log 什么时候刷盘？

缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？

主要有下面几个时机：

- MySQL 正常关闭时；
- 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
- InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。
- 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。

**innodb_flush_log_at_trx_commit 参数控制的是什么？**

单独执行一个更新语句的时候，InnoDB 引擎会自己启动一个事务，在执行更新语句的过程中，生成的 redo log 先写入到 redo log buffer 中，然后等事务提交的时候，再将缓存在 redo log buffer 中的 redo log 按组的方式「顺序写」到磁盘。

上面这种 redo log 刷盘时机是在事务提交的时候，这个默认的行为。

除此之外，InnoDB 还提供了另外两种策略，由参数 `innodb_flush_log_at_trx_commit` 参数控制，可取的值有：0、1、2，默认值为 1，这三个值分别代表的策略如下：

- 当设置该**参数为 0 时**，表示每次事务提交时 ，还是**将 redo log 留在 redo log buffer 中** ，该模式下在事务提交时不会主动触发写入磁盘的操作。
- 当设置该**参数为 1 时**，表示每次事务提交时，都**将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘**，这样可以保证 MySQL 异常重启之后数据不会丢失。
- 当设置该**参数为 2 时**，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log **写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘**，因为操作系统的文件系统中有个 Page Cache，Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。

**innodb_flush_log_at_trx_commit 为 0 和 2 的时候，什么时候才将 redo log 写入磁盘？**

InnoDB 的后台线程每隔 1 秒：

- 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 `write()` 写到操作系统的 Page Cache，然后调用 `fsync()` 持久化到磁盘。**所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失**;
- 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。**所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失**。

**这三个参数的应用场景是什么？**

这三个参数的数据安全性和写入性能的比较如下：

- 数据安全性：参数 1 > 参数 2 > 参数 0
- 写入性能：参数 0 > 参数 2> 参数 1

所以，数据安全性和写入性能是熊掌不可得兼的，**要不追求数据安全性，牺牲性能；要不追求性能，牺牲数据安全性**。

- 在一些对数据安全性要求比较高的场景中，显然 `innodb_flush_log_at_trx_commit` 参数需要设置为 1。
- 在一些可以容忍数据库崩溃时丢失 1s 数据的场景中，我们可以将该值设置为 0，这样可以明显地减少日志同步到磁盘的 I/O 操作。
- 安全性和性能折中的方案就是参数 2，虽然参数 2 没有参数 0 的性能高，但是数据安全性方面比参数 0 强，因为参数 2 只要操作系统不宕机，即使数据库崩溃了，也不会丢失数据，同时性能方便比参数 1 高。

#### redo log 文件写满了怎么办？

默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：`ib_logfile0` 和 `ib_logfile1` 。

在重做日志组中，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。

重做日志文件组是以**循环写**的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。

所以 InnoDB 存储引擎会先写 ib_logfile0 文件，当 ib_logfile0 文件被写满的时候，会切换至 ib_logfile1 文件，当 ib_logfile1 文件也被写满时，会切换回 ib_logfile0 文件。

我们知道 redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。

redo log 是循环写的方式，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置。

图中的：

- write pos 和 checkpoint 的移动都是顺时针方向；
- write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作；
- check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录；

如果 write pos 追上了 checkpoint，就意味着 **redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞**（*因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要*），此时**会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针）**，然后 MySQL 恢复正常运行，继续执行新的更新操作。

所以，一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程。

### 为什么需要 binlog ？

前面介绍的 undo log 和 redo log 这两个日志都是 Innodb 存储引擎生成的。

MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。

binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。

**为什么有了 binlog， 还要有 redo log？**

这个问题跟 MySQL 的时间线有关系。

最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。

而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力。

#### redo log 和 binlog 有什么区别？

这两个日志有四个区别。

*1、适用对象不同：*

- binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；
- redo log 是 Innodb 存储引擎实现的日志；

*2、文件格式不同：*

- binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下：
  - STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；
  - ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；
  - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；
- redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

*3、写入方式不同：*

- binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。
- redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。

*4、用途不同：*

- binlog 用于备份恢复、主从复制；
- redo log 用于掉电等故障恢复。

**如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？**

不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。

因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。

binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。

#### 主从复制是怎么实现？

MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。

这个过程一般是**异步**的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。

MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：主库写 binlog 日志，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。
- **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。

具体详细过程如下：

- MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
- 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
- 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。

在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。

**从库是不是越多越好？**

不是的。

因为从库数量增加，从库连接上来的 I/O 线程也比较多，**主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽**。

所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。

**MySQL 主从复制还有哪些模型？**

主要有三种：

- **同步复制**：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。
- **异步复制**（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。
- **半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**。

#### binlog 什么时候刷盘？

事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 binlog cache 写到 binlog 文件中。

一个事务的 binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入。这是因为有一个线程只能同时有一个事务在执行的设定，所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的。

MySQL 给每个线程分配了一片内存用于缓冲 binlog ，该内存叫 binlog cache，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。

**什么时候 binlog cache 会写到 binlog 文件？**

在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。

虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件：

- 图中的 write，指的就是指把日志写入到 binlog 文件，但是并没有把数据持久化到磁盘，因为数据还缓存在文件系统的 page cache 里，write 的写入速度还是比较快的，因为不涉及磁盘 I/O。
- 图中的 fsync，才是将数据持久化到磁盘的操作，这里就会涉及磁盘 I/O，所以频繁的 fsync 会导致磁盘的 I/O 升高。

MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率：

- sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘；
- sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync；
- sync_binlog =N(N>1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。

在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。

而当 sync_binlog 设置为 1 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。

如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。

**三个日志讲完了，至此我们可以先小结下，update 语句的执行过程。**

当优化器分析出成本最小的执行计划后，执行器就按照执行计划开始进行更新操作。

具体更新一条记录 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 的流程如下:

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：
   - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；
3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。
4. InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 **WAL 技术**，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。
5. 至此，一条记录更新完了。
6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。
7. 事务提交，剩下的就是「两阶段提交」的事情了，接下来就讲这个。

### 为什么需要两阶段提交？

事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。

举个例子，假设 id = 1 这行数据的字段 name 的值原本是 'jay'，然后执行 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况：

- **如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入**。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性；
- **如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入**。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性；

可以看到，在持久化 redo log 和 binlog 这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。这是因为 redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。

**MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决**，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。

**两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。注意，不要把提交（Commit）阶段和 commit 语句混淆了，commit 语句执行的时候，会包含提交（Commit）阶段。

举个拳击比赛的例子，两位拳击手（参与者）开始比赛之前，裁判（协调者）会在中间确认两位拳击手的状态，类似于问你准备好了吗？

- **准备阶段**：裁判（协调者）会依次询问两位拳击手（参与者）是否准备好了，然后拳击手听到后做出应答，如果觉得自己准备好了，就会跟裁判说准备好了；如果没有自己还没有准备好（比如拳套还没有带好），就会跟裁判说还没准备好。
- **提交阶段**：如果两位拳击手（参与者）都回答准备好了，裁判（协调者）宣布比赛正式开始，两位拳击手就可以直接开打；如果任何一位拳击手（参与者）回答没有准备好，裁判（协调者）会宣布比赛暂停，对应事务中的回滚操作。

#### 两阶段提交的过程是怎样的？

在 MySQL 的 InnoDB 存储引擎中，开启 binlog 的情况下，MySQL 会同时维护 binlog 日志与 InnoDB 的 redo log，为了保证这两个日志的一致性，MySQL 使用了**内部 XA 事务**（是的，也有外部 XA 事务，跟本文不太相关，我就不介绍了），内部 XA 事务由 binlog 作为协调者，存储引擎是参与者。

当客户端执行 commit 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，**分两阶段来完成 XA 事务的提交**

可以知道，事务的提交过程有两个阶段，就是**将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog**，具体如下：

- **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；
- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；

#### 异常重启会出现什么现象？

我们来看看在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？

不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），还是时刻 B （redo log 和 binlog 都已经写入磁盘，还没写入 commit 标识）崩溃，**此时的 redo log 都处于 prepare 状态**。

在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID：

- **如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务**。对应时刻 A 崩溃恢复的情况。
- **如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务**。对应时刻 B 崩溃恢复的情况。

可以看到，**对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID**，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。

所以说，**两阶段提交是以 binlog 写成功为事务提交成功的标识**，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。

**处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计?**

binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

**事务没提交的时候，redo log 会被持久化到磁盘吗？**

会的。

事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。

也就是说，**事务没提交的时候，redo log 也是可能被持久化到磁盘的**。

有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？

放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。

所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。

### 两阶段提交有什么问题？

两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响：

- **磁盘 I/O 次数高**：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。
- **锁竞争激烈**：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。

**为什么两阶段提交的磁盘 I/O 次数会很高？**

binlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一般我们为了避免日志丢失的风险，会将这两个参数设置为 1：

- 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘；
- 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘；

可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会**至少调用 2 次刷盘操作**，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。

**为什么锁竞争激烈？**

在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。

通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。

#### 组提交

**MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

- **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
- **sync 阶段**：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
- **commit 阶段**：各个事务按顺序做 InnoDB commit 操作；

上面的**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。

对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。

**有 binlog 组提交，那有 redo log 组提交吗？**

这个要看 MySQL 版本，MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。

在 MySQL 5.6 的组提交逻辑中，每个事务各自执行 prepare 阶段，也就是各自将 redo log 刷盘，这样就没办法对 redo log 进行组提交。

所以在 MySQL 5.7 版本中，做了个改进，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。

这个优化是将 redo log 的刷盘延迟到了 flush 阶段之中，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。

接下来介绍每个阶段的过程，注意下面的过程针对的是“双 1” 配置（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）。

### MySQL 磁盘 I/O 很高，有什么优化的方法？

现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
- 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。

### 总结

具体更新一条记录`UPDATE t_user SET name = 'xiaolin' WHERE id = 1;`的流程如下：

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取id=1这条记录：
   - 如果id=1这一行所在的数据页本来就在buffer pool中，就直接返回给执行器更新；
   - 如果记录不在buffer pool，将数据页从磁盘读入到buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新之前的记录和更新之后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把跟新前的记录和更新后的记录都当作参数传给InnoDB层，让InnoDB真正的执行更新记录的操作；
3. 开始事务，InnoDB层更新记录前，首先要记录相应的undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条undo log，undo log会吸入buffer pool的Undo页面，不过在内存修改该Undo页面后，需要记录对应的redo log。
4. InnoDB层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到redo log里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是**WAL技术**，MySQL的写操作并不是立刻写到磁盘上，而是先写redo日志，然后在合适的时间再将修改的行数据写到磁盘上。
5. 至此，一条记录更新完了。
6. 在一条更新记录执行完成后，然后开始记录该语句的binlog，此时记录的binlog会被保存到binlog cache，并没有刷新到硬盘上的binlog文件，在事务提交时才会统一该事务运行过程中的所有binlog刷新到硬盘。
7. 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）：
   - **prepare 阶段**：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；
   - **commit 阶段**：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；
8. 至此，一条更新语句执行完成。

## 内存篇

### 揭开Buffer Pool的面纱

#### 为什么要有Buffer Pool？

虽然说MySQL的数据是存储在磁盘里的，但是也不能每次都从磁盘里读取数据，这样性能是极差的。

要想提升查询性能，加个缓存就行了嘛。所以，当数据从磁盘中取出后，缓存到内存中，下次查询同样的数据的时候，直接从内存中读取。

为此，InnoDB存储引擎设计了一个**缓冲池（Buffer Pool），**来提升数据库的读写性能。

有了缓冲池后：

- 当读取数据时，如果数据存在于Buffer Pool中，客户端就会直接读取Buffer Pool中的数据，否则再去磁盘中读取。
- 当修改数据时，首先是修改Buffer Pool中数据所在的页，然后将其页设置为脏页，最后由后台线程将脏页写入到磁盘。

##### Buffer Pool有多大？

Buffer Pool是在MySQL启动的时候，向操作系统申请的一篇连续的内存空间，默认配置写Buffer Pool只有`128MB`。

可以通过调整`innodb_buffer_pool_size`参数来设置Buffer Pool的大小，一般建议设置成可用物理内存的60%~80%。

##### Buffer Pool缓存什么？

InnoDB会把存储的数据划分成若干个[页]，以页作为磁盘和内存交互的基本空间，一个页的默认大小为16KB。因此，Buffer Pool同样需要按[页]来划分。

在MySQL启动的时候，**InnoDB会为Buffer Pool申请一片连续的内存空间，然后按照默认的`16KBb`的大小划分出一个个的页，Buffer Pool中的页就叫做缓存页。**此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到Buffer Pool中。

所以，MySQL刚启动时，你会观察到使用的虚拟内存空间很大，而使用到的物理内存空间却很小，这是因为只有虚拟内存被访问后，操作系统才会触发缺页中断，接着将虚拟地址和物理地址建立映射关系。

Buffer Pool除了缓存[索引页]和[数据页]，还包括了undo页，插入缓存、自适应哈希索引、锁信息等等。

为了更好的管理这些Buffer Pool中的缓存页，InnoDB为每一个缓存页都创建了一个**控制块**，控制块信息包括[缓存页的表空间、页号、缓存页地址、链表节点]等等。

控制块也是占有内存空间的，它是放在Buffer Pool的最前面，接着才是缓存页。

控制块和缓存页之间灰色部分称为碎片空间。

**为什么会有碎片空间呢？**

你想想啊，每一个控制块都对应一个缓存页，那在分配足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，自然就用不到咯，这个用不到的那点内存空间就被称为碎片了。

当然，如果你把Buffer Pool的大小设置的刚刚好的话，也可能不会产生碎片。

**查询一条记录，就只需要缓冲一条记录吗？**

不是的。

当我们查询一条记录时，InnoDB是会把整个页的数据加载到Buffer Pool中，因为，通过所以只能定位到磁盘中的页，而不能定位到页中的一条记录。将页加载到Buffer Pool后，再通过页里的页目录去定位到某条具体的记录。

#### 如何管理Buffer Pool？

##### 如何管理空闲页？

Buffer Pool是一片连续的内存空间，当MySQL运行一段时间后，这片连续的内存空间中的缓存页既有空闲的，也有被使用的。

当我们从磁盘读取数据的时候，总不能通过遍历这篇连续的内存空间来找到空闲的缓存页吧，这样效率太低了。

所以，为了能够快速找到空闲的缓存页，可以使用链表结构，将空闲缓存页的[控制块]作为链表的节点，这个链表称为**Free链表**（空闲链表）。

Free链表上除了有控制块，还有一个头节点，该头节点包含链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。

Free 链表节点是一个一个的控制块，而每个控制块包含着对应缓存页的地址，所以相当于 Free 链表节点都对应一个空闲的缓存页。

有了 Free 链表后，每当需要从磁盘中加载一个页到 Buffer Pool 中时，就从 Free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上，然后把该缓存页对应的控制块从 Free 链表中移除。

##### 如何管理脏页？

设计 Buffer Pool 除了能提高读性能，还能提高写性能，也就是更新数据的时候，不需要每次都要写入磁盘，而是将 Buffer Pool 对应的缓存页标记为**脏页**，然后再由后台线程将脏页写入到磁盘。

那为了能快速知道哪些缓存页是脏的，于是就设计出 **Flush 链表**，它跟 Free 链表类似的，链表的节点也是控制块，区别在于 Flush 链表的元素都是脏页。

有了 Flush 链表后，后台线程就可以遍历 Flush 链表，将脏页写入到磁盘。

##### 如何提高缓存命中率？

Buffer Pool 的大小是有限的，对于一些频繁访问的数据我们希望可以一直留在 Buffer Pool 中，而一些很少访问的数据希望可以在某些时机可以淘汰掉，从而保证 Buffer Pool 不会因为满了而导致无法再缓存新的数据，同时还能保证常用数据留在 Buffer Pool 中。

要实现这个，最容易想到的就是 LRU（Least recently used）算法。

该算法的思路是，链表头部的节点是最近使用的，而链表末尾的节点是最久没被使用的。那么，当空间不够了，就淘汰最久没被使用的节点，从而腾出空间。

简单的 LRU 算法的实现思路是这样的：

- 当访问的页在 Buffer Pool 里，就直接把该页对应的 LRU 链表节点移动到链表的头部。
- 当访问的页不在 Buffer Pool 里，除了要把页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点。

到这里我们可以知道，Buffer Pool 里有三种页和链表来管理数据：

- Free Page（空闲页），表示此页未被使用，位于 Free 链表；
- Clean Page（干净页），表示此页已被使用，但是页面未发生修改，位于LRU 链表。
- Dirty Page（脏页），表示此页「已被使用」且「已经被修改」，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。脏页同时存在于 LRU 链表和 Flush 链表。

简单的 LRU 算法并没有被 MySQL 使用，因为简单的 LRU 算法无法避免下面这两个问题：

- 预读失效；
- Buffer Pool 污染；

**什么是预读失效？**

先来说说 MySQL 的预读机制。程序是有空间局部性的，靠近当前被访问数据的数据，在未来很大概率会被访问到。

所以，MySQL 在加载数据页时，会提前把它相邻的数据页一并加载进来，目的是为了减少磁盘 IO。

但是可能这些**被提前加载进来的数据页，并没有被访问**，相当于这个预读是白做了，这个就是**预读失效**。

如果使用简单的 LRU 算法，就会把预读页放到 LRU 链表头部，而当 Buffer Pool空间不够的时候，还需要把末尾的页淘汰掉。

如果这些预读页如果一直不会被访问到，就会出现一个很奇怪的问题，不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是频繁访问的页，这样就大大降低了缓存命中率。

**怎么解决预读失效而导致缓存命中率降低的问题？**

我们不能因为害怕预读失效，而将预读机制去掉，大部分情况下，局部性原理还是成立的。

要避免预读失效带来影响，最好就是**让预读的页停留在 Buffer Pool 里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长**。

那到底怎么才能避免呢？

MySQL 是这样做的，它改进了 LRU 算法，将 LRU 划分了 2 个区域：**old 区域 和 young 区域**。

young 区域在 LRU 链表的前半部分，old 区域则是在后半部分

old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pct` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。

**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。

虽然通过划分 old 区域 和 young 区域避免了预读失效带来的影响，但是还有个问题无法解决，那就是 Buffer Pool 污染的问题。

**什么是 Buffer Pool 污染？**

当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为 **Buffer Pool 污染**。

注意， Buffer Pool 污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成 Buffer Pool 污染。

比如，在一个数据量非常大的表，执行了这条语句：

```sql
select * from t_user where name like "%xiaolin%";
```

可能这个查询出来的结果就几条记录，但是由于这条语句会发生索引失效，所以这个查询过程是全表扫描的，接着会发生如下的过程：

- 从磁盘读到的页加入到 LRU 链表的 old 区域头部；
- 当从页里读取行记录时，也就是页被访问的时候，就要将该页放到 young 区域头部；
- 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；
- 如此往复，直到扫描完表中的所有记录。

经过这一番折腾，原本 young 区域的热点数据都会被替换掉。

**怎么解决出现 Buffer Pool 污染而导致缓存命中率下降的问题？**

像前面这种全表扫描的查询，很多缓冲页其实只会被访问一次，但是它却只因为被访问了一次而进入到 young 区域，从而导致热点数据被替换了。

LRU 链表中 young 区域就是热点数据，只要我们提高进入到 young 区域的门槛，就能有效地保证 young 区域里的热点数据不会被替换掉。

MySQL 是这样做的，进入到 young 区域条件增加了一个**停留在 old 区域的时间判断**。

具体是这样做的，在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：

- 如果后续的访问时间与第一次访问的时间**在某个时间间隔内**，那么**该缓存页就不会被从 old 区域移动到 young 区域的头部**；
- 如果后续的访问时间与第一次访问的时间**不在某个时间间隔内**，那么**该缓存页移动到 young 区域的头部**；

这个间隔时间是由 `innodb_old_blocks_time` 控制的，默认是 1000 ms。

也就说，**只有同时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部**，这样就解决了 Buffer Pool 污染的问题 。

另外，MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点频繁移动到头部。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。

##### 脏页什么时候会被刷入磁盘？

引入了 Buffer Pool 后，当修改数据时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，但是磁盘中还是原数据。

因此，脏页需要被刷入磁盘，保证缓存和磁盘数据一致，但是若每次修改数据都刷入磁盘，则性能会很差，因此一般都会在一定时机进行批量刷盘。

可能大家担心，如果在脏页还没有来得及刷入到磁盘时，MySQL 宕机了，不就丢失数据了吗？

这个不用担心，InnoDB 的更新操作采用的是 Write Ahead Log 策略，即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力。

下面几种情况会触发脏页的刷新：

- 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；
- Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
- MySQL 认为空闲时，后台线程会定期将适量的脏页刷入到磁盘；
- MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；

在我们开启了慢 SQL 监控后，如果你发现**「偶尔」会出现一些用时稍长的 SQL**，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。

如果间断出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。

#### 总结

InnoDB存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。

Buffer Pool以页为单位缓冲数据，可以通过`innodb_buffer_pool_size`参数调整缓冲池的大小，默认是128M。

InnoDB通过三种链表来管理缓页：

- Free List（空闲页链表），管理空闲页；
- Flush List（脏页链表），管理脏页；
- LRU List，管理脏页+干净页，将最近经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。

InnoDB对LRU做了一些优化，我们熟悉的LRU算法通常是将最近查询的数据放到LRU链表的头部，而InnoDB做了2点优化：

- 将LRU链表分为**young和old两个区域，**加入缓冲池的页，优先插入old区域；页被访问时，才进入young区域，目的是为了解决于都失效的问题。
- 将**[页被访问]且[old区域停留时间超过`innodb_old_blocks_time阈值（默认为1秒）`]**时，才会将页插入到young区域，否则还是插入到old区域，目的时为了解决批量数据访问，大量热数据淘汰的问题。

可以通过调整`innodb_old_blocks_pct`参数，设置young区域和old区域的比例。

在开启了慢SQL监控后，如果你发现[偶尔]会出现一些用时稍长的SQL，这可能因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大Buffer Pool空间或redo log日志的大小。